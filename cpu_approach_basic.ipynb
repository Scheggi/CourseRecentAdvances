{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:29:29.913408Z",
     "start_time": "2025-06-22T18:29:29.909341Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import argparse, contextlib\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:29:30.559201Z",
     "start_time": "2025-06-22T18:29:30.554508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW = 50\n",
    "STRIDE = 25\n",
    "LOCATIONS = [\"right_arm\", \"left_arm\", \"right_leg\", \"left_leg\"]\n",
    "AXES = [\"x\", \"y\", \"z\"]\n",
    "data_dir = Path('data')\n",
    "train_dir = data_dir / 'train'\n",
    "meta_file = data_dir / 'meta_data.txt'\n",
    "test_file = data_dir/'test.csv'\n",
    "workdir = Path('work')\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "num_classes = len(label_map)\n",
    "C = 3\n",
    "crit = nn.CrossEntropyLoss()"
   ],
   "id": "30569a70b211ddb7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:32:49.730486Z",
     "start_time": "2025-06-22T18:32:49.721870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_locations() -> list[str]:\n",
    "    files = sorted(train_dir.glob('sbj_*.csv'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No 'sbj_*.csv' files found in {data_dir}\")\n",
    "    sample = pd.read_csv(files[0], nrows=0)\n",
    "    locs = sorted({col.split('_acc_')[0] for col in sample.columns if '_acc_' in col})\n",
    "    if not locs:\n",
    "        raise ValueError(f\"No sensor columns found in {files[0]}\")\n",
    "    return locs\n",
    "    sample = pd.read_csv(next(data_dir.glob('sbj_*.csv')), nrows=0)\n",
    "    return sorted({col.split('_acc_')[0] for col in sample.columns if '_acc_' in col})\n",
    "\n",
    "def cols_for(loc: str) -> List[str]:\n",
    "    return [f\"{loc}_acc_{ax}\" for ax in AXES]\n",
    "\n",
    "def make_clean_split(df: pd.DataFrame, loc: str, want_label: bool) -> pd.DataFrame:\n",
    "    cols = cols_for(loc)\n",
    "    cleaned = (\n",
    "        df[cols + (['label'] if want_label else [])]\n",
    "          .rename(columns=dict(zip(cols, AXES)))\n",
    "          .dropna()\n",
    "    )\n",
    "    if want_label:\n",
    "        return cleaned[AXES + ['label']].reset_index(drop=True)\n",
    "    return cleaned[AXES].reset_index(drop=True)\n",
    "\n",
    "def preprocess() -> List[str]:\n",
    "    locs = detect_locations()\n",
    "    print(locs)\n",
    "    ds_dir = workdir / 'datasets'\n",
    "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for loc in locs:\n",
    "        parts = []\n",
    "        need = cols_for(loc) + ['label']\n",
    "        for f in sorted(train_dir.glob('sbj_*.csv')):\n",
    "            df = pd.read_csv(f)\n",
    "            if not all(c in df.columns for c in need):\n",
    "                continue\n",
    "            parts.append(make_clean_split(df, loc, True))\n",
    "        if parts:\n",
    "            pd.concat(parts).to_csv(ds_dir / f'train_{loc}.csv', index=False)\n",
    "\n",
    "\n",
    "    test_raw = pd.read_csv(test_file)\n",
    "    for loc in LOCATIONS:\n",
    "        if all(c in test_raw.columns for c in cols_for(loc)):\n",
    "            make_clean_split(test_raw, loc, False).to_csv(\n",
    "                ds_dir / f'test_{loc}.csv', index=False)\n",
    "\n",
    "    return locs\n"
   ],
   "id": "12d3818b5e6608e0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:34:09.313743Z",
     "start_time": "2025-06-22T18:32:51.279577Z"
    }
   },
   "cell_type": "code",
   "source": "preprocess()",
   "id": "45e34d761fef76d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left_arm', 'left_leg', 'right_arm', 'right_leg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193334/1878972973.py:37: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n",
      "/tmp/ipykernel_193334/1878972973.py:37: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n",
      "/tmp/ipykernel_193334/1878972973.py:37: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n",
      "/tmp/ipykernel_193334/1878972973.py:37: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left_arm', 'left_leg', 'right_arm', 'right_leg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:31:32.471126Z",
     "start_time": "2025-06-22T18:31:32.463608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def segment_windows(data: np.ndarray, labels: np.ndarray|None, window:int=WINDOW, stride:int=STRIDE) -> Tuple[np.ndarray, np.ndarray|None]:\n",
    "    X_list, y_list = [], []\n",
    "    for s in range(0, len(data)-window+1, stride):\n",
    "        seg = data[s:s+window]\n",
    "        if np.isnan(seg).any(): continue\n",
    "        X_list.append(seg)\n",
    "        if labels is not None:\n",
    "            lbl = labels[s:s+window]\n",
    "            vals,cnts = np.unique(lbl, return_counts=True)\n",
    "            y_list.append(vals[cnts.argmax()])\n",
    "    X = np.stack(X_list) if X_list else np.empty((0,window,3),dtype=np.float32)\n",
    "    y = np.array(y_list) if labels is not None else None\n",
    "    return X, y\n",
    "\n",
    "class WearDS(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, scaler: StandardScaler):\n",
    "        self.X = scaler.transform(X.reshape(-1,3)).reshape(X.shape)\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.X[i]).permute(1,0), self.y[i]\n",
    "\n",
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self, n_classes:int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(3,64,5,padding=2), nn.ReLU(),\n",
    "            nn.Conv1d(64,64,5,padding=2), nn.ReLU(), nn.Dropout(0.2))\n",
    "        self.lstm = nn.LSTM(64,128,batch_first=True)\n",
    "        self.fc   = nn.Sequential(nn.Dropout(0.5), nn.Linear(128,n_classes))\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        _,(h,_) = self.lstm(x)\n",
    "        return self.fc(h[-1])"
   ],
   "id": "2e6efdc54f06465",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:34:27.341563Z",
     "start_time": "2025-06-22T18:34:27.326398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, loader, crit, opt, scaler_amp, device):\n",
    "    model.train(); tot=correct=ls=0.0\n",
    "    amp_ctx = autocast() if device=='cuda' else contextlib.nullcontext()\n",
    "    for X,y in loader:\n",
    "        X = X.to(device); y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        with amp_ctx:\n",
    "            out=model(X); loss=crit(out,y)\n",
    "        if scaler_amp:\n",
    "            scaler_amp.scale(loss).backward(); scaler_amp.step(opt); scaler_amp.update()\n",
    "        else:\n",
    "            loss.backward(); opt.step()\n",
    "        preds=out.argmax(1)\n",
    "        tot+=y.size(0); correct+=(preds==y).sum().item(); ls+=loss.item()*y.size(0)\n",
    "    return ls/tot, correct/tot\n",
    "\n",
    "def train_locations(work_dir:Path, locs:List[str], epochs:int, batch:int, device:str):\n",
    "    mdl_dir = work_dir/'models'; mdl_dir.mkdir(exist_ok=True)\n",
    "    for loc in locs:\n",
    "        csv = work_dir/'datasets'/f'train_{loc}.csv'\n",
    "        df = pd.read_csv(csv)\n",
    "        X,y = segment_windows(df[AXES].values.astype(np.float32), df['label'].values)\n",
    "        if X.size==0: continue\n",
    "        # encode\n",
    "        classes = sorted(set(y)); lbl2idx={c:i for i,c in enumerate(classes)}\n",
    "        y_enc = np.array([lbl2idx[v] for v in y],dtype=np.int64)\n",
    "        scaler = StandardScaler().fit(X.reshape(-1,3))\n",
    "        ds = WearDS(X,y_enc,scaler)\n",
    "        dl = DataLoader(ds,batch_size=batch,shuffle=True,\n",
    "                        pin_memory=(device=='cuda'),num_workers=0)\n",
    "        model=DeepConvLSTM(len(classes)).to(device)\n",
    "        w=compute_class_weight('balanced',classes=np.unique(y_enc),y=y_enc)\n",
    "        crit=nn.CrossEntropyLoss(weight=torch.tensor(w,dtype=torch.float).to(device))\n",
    "        opt=torch.optim.AdamW(model.parameters(),1e-3,weight_decay=1e-4)\n",
    "        amp=GradScaler() if device=='cuda' else None\n",
    "        best=0.0\n",
    "        for ep in range(1,epochs+1):\n",
    "            loss,acc=train_epoch(model,dl,crit,opt,amp,device)\n",
    "            print(f\"[{loc}] {ep}/{epochs} loss={loss:.4f} acc={acc:.4f}\")\n",
    "            if acc>best:\n",
    "                best=acc\n",
    "                ckpt={'model':model.state_dict(),'scaler':scaler,'label_map':lbl2idx}\n",
    "                torch.save(ckpt, mdl_dir/f'{loc}_best.pt')\n",
    "        del model,dl,ds,opt,crit,amp\n"
   ],
   "id": "2650f4c6255b158a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T20:05:57.351656Z",
     "start_time": "2025-06-22T18:36:29.103072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_locs = detect_locations()\n",
    "train_locations(workdir, all_locs, 20, 256, 'cpu')"
   ],
   "id": "78d03af63f81336e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[left_arm] 1/20 loss=1.7030 acc=0.4388\n",
      "[left_arm] 2/20 loss=1.2990 acc=0.5672\n",
      "[left_arm] 3/20 loss=1.1476 acc=0.6190\n",
      "[left_arm] 4/20 loss=1.0661 acc=0.6459\n",
      "[left_arm] 5/20 loss=1.0026 acc=0.6685\n",
      "[left_arm] 6/20 loss=0.9658 acc=0.6797\n",
      "[left_arm] 7/20 loss=0.9220 acc=0.6952\n",
      "[left_arm] 8/20 loss=0.8849 acc=0.7072\n",
      "[left_arm] 9/20 loss=0.8530 acc=0.7176\n",
      "[left_arm] 10/20 loss=0.8368 acc=0.7206\n",
      "[left_arm] 11/20 loss=0.8078 acc=0.7314\n",
      "[left_arm] 12/20 loss=0.7790 acc=0.7419\n",
      "[left_arm] 13/20 loss=0.7652 acc=0.7449\n",
      "[left_arm] 14/20 loss=0.7424 acc=0.7521\n",
      "[left_arm] 15/20 loss=0.7306 acc=0.7544\n",
      "[left_arm] 16/20 loss=0.7055 acc=0.7623\n",
      "[left_arm] 17/20 loss=0.6933 acc=0.7672\n",
      "[left_arm] 18/20 loss=0.6833 acc=0.7705\n",
      "[left_arm] 19/20 loss=0.6794 acc=0.7719\n",
      "[left_arm] 20/20 loss=0.6579 acc=0.7796\n",
      "[left_leg] 1/20 loss=1.5423 acc=0.4431\n",
      "[left_leg] 2/20 loss=1.1718 acc=0.5457\n",
      "[left_leg] 3/20 loss=1.0915 acc=0.5723\n",
      "[left_leg] 4/20 loss=1.0153 acc=0.6021\n",
      "[left_leg] 5/20 loss=0.9530 acc=0.6289\n",
      "[left_leg] 6/20 loss=0.9088 acc=0.6461\n",
      "[left_leg] 7/20 loss=0.8778 acc=0.6571\n",
      "[left_leg] 8/20 loss=0.8372 acc=0.6698\n",
      "[left_leg] 9/20 loss=0.8258 acc=0.6766\n",
      "[left_leg] 10/20 loss=0.7981 acc=0.6849\n",
      "[left_leg] 11/20 loss=0.7819 acc=0.6933\n",
      "[left_leg] 12/20 loss=0.7618 acc=0.7008\n",
      "[left_leg] 13/20 loss=0.7435 acc=0.7081\n",
      "[left_leg] 14/20 loss=0.7311 acc=0.7128\n",
      "[left_leg] 15/20 loss=0.7259 acc=0.7166\n",
      "[left_leg] 16/20 loss=0.7047 acc=0.7240\n",
      "[left_leg] 17/20 loss=0.6918 acc=0.7292\n",
      "[left_leg] 18/20 loss=0.6794 acc=0.7352\n",
      "[left_leg] 19/20 loss=0.6681 acc=0.7398\n",
      "[left_leg] 20/20 loss=0.6625 acc=0.7414\n",
      "[right_arm] 1/20 loss=1.7250 acc=0.4280\n",
      "[right_arm] 2/20 loss=1.3256 acc=0.5623\n",
      "[right_arm] 3/20 loss=1.1829 acc=0.6079\n",
      "[right_arm] 4/20 loss=1.1009 acc=0.6328\n",
      "[right_arm] 5/20 loss=1.0358 acc=0.6567\n",
      "[right_arm] 6/20 loss=0.9844 acc=0.6716\n",
      "[right_arm] 7/20 loss=0.9407 acc=0.6856\n",
      "[right_arm] 8/20 loss=0.9068 acc=0.6984\n",
      "[right_arm] 9/20 loss=0.8713 acc=0.7086\n",
      "[right_arm] 10/20 loss=0.8411 acc=0.7176\n",
      "[right_arm] 11/20 loss=0.8134 acc=0.7259\n",
      "[right_arm] 12/20 loss=0.7913 acc=0.7339\n",
      "[right_arm] 13/20 loss=0.7746 acc=0.7382\n",
      "[right_arm] 14/20 loss=0.7519 acc=0.7466\n",
      "[right_arm] 15/20 loss=0.7342 acc=0.7516\n",
      "[right_arm] 16/20 loss=0.7144 acc=0.7580\n",
      "[right_arm] 17/20 loss=0.7066 acc=0.7603\n",
      "[right_arm] 18/20 loss=0.6866 acc=0.7674\n",
      "[right_arm] 19/20 loss=0.6757 acc=0.7706\n",
      "[right_arm] 20/20 loss=0.6628 acc=0.7767\n",
      "[right_leg] 1/20 loss=1.4281 acc=0.4767\n",
      "[right_leg] 2/20 loss=1.1013 acc=0.5701\n",
      "[right_leg] 3/20 loss=1.0269 acc=0.5969\n",
      "[right_leg] 4/20 loss=0.9801 acc=0.6111\n",
      "[right_leg] 5/20 loss=0.9521 acc=0.6234\n",
      "[right_leg] 6/20 loss=0.9105 acc=0.6392\n",
      "[right_leg] 7/20 loss=0.8764 acc=0.6523\n",
      "[right_leg] 8/20 loss=0.8652 acc=0.6558\n",
      "[right_leg] 9/20 loss=0.8274 acc=0.6720\n",
      "[right_leg] 10/20 loss=0.8120 acc=0.6763\n",
      "[right_leg] 11/20 loss=0.7932 acc=0.6861\n",
      "[right_leg] 12/20 loss=0.7775 acc=0.6915\n",
      "[right_leg] 13/20 loss=0.7562 acc=0.6999\n",
      "[right_leg] 14/20 loss=0.7499 acc=0.7048\n",
      "[right_leg] 15/20 loss=0.7328 acc=0.7091\n",
      "[right_leg] 16/20 loss=0.7177 acc=0.7154\n",
      "[right_leg] 17/20 loss=0.7005 acc=0.7234\n",
      "[right_leg] 18/20 loss=0.6914 acc=0.7279\n",
      "[right_leg] 19/20 loss=0.6791 acc=0.7319\n",
      "[right_leg] 20/20 loss=0.6656 acc=0.7385\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T22:01:21.542730Z",
     "start_time": "2025-06-22T22:00:31.766907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def predict_and_save(\n",
    "    test_csv: Path,\n",
    "    models_dir: Path,\n",
    "    output_csv: Path,\n",
    "    axes: List[str] = ['x_axis', 'y_axis', 'z_axis'],\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    df = pd.read_csv(test_csv)\n",
    "    results = []\n",
    "    for loc, grp in df.groupby('sensor_location', sort=False):\n",
    "        print(f\"→ Predicting location '{loc}' ({len(grp)} rows)\")\n",
    "        ckpt_path = models_dir / f\"{loc}_best.pt\"\n",
    "        if not ckpt_path.exists():\n",
    "            raise FileNotFoundError(f\"Model for '{loc}' not found at {ckpt_path}\")\n",
    "\n",
    "        with torch.serialization.safe_globals([StandardScaler]):\n",
    "            ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "\n",
    "        num_classes = len(ckpt['label_map'])\n",
    "        model = DeepConvLSTM(num_classes).to(device)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        model.eval()\n",
    "        scaler: StandardScaler = ckpt['scaler']\n",
    "        inv_map = {v: k for k, v in ckpt['label_map'].items()}\n",
    "\n",
    "        windows = []\n",
    "        for _, row in grp.iterrows():\n",
    "            lists = [ast.literal_eval(row[c]) for c in axes]\n",
    "            arr = np.stack(lists, axis=0).T.astype('float32')  # (window_len, 3)\n",
    "            windows.append(arr)\n",
    "        X = np.stack(windows, axis=0)  # (n_loc, window_len, 3)\n",
    "\n",
    "        n, L, C = X.shape\n",
    "        Xs = scaler.transform(X.reshape(-1, C)).reshape(n, L, C)\n",
    "\n",
    "        ds = WearDS(X, Xs, scaler)\n",
    "        dl = DataLoader(ds, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in dl:\n",
    "                if isinstance(xb, list):\n",
    "                    xb = np.stack(xb, axis=0)\n",
    "                xb = torch.tensor(xb, dtype=torch.float32, device=device)\n",
    "                out = model(xb)\n",
    "                for pi in out.argmax(dim=1).cpu().numpy():\n",
    "                    lbl = inv_map[int(pi)]\n",
    "                    preds.append(LABEL_MAP[lbl])\n",
    "\n",
    "        for (_, row), p in zip(grp.iterrows(), preds):\n",
    "            results.append((row['id'], p))\n",
    "\n",
    "    res_df = pd.DataFrame(results, columns=['id', 'label'])\n",
    "    res_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Saved predictions to {output_csv}\")\n",
    "\n",
    "predict_and_save(test_file,workdir/'models', workdir/'result.csv')"
   ],
   "id": "445320a88c292816",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicting location 'right_arm' (12234 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193334/3884471167.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb = torch.tensor(xb, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicting location 'left_arm' (12234 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193334/3884471167.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb = torch.tensor(xb, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicting location 'right_leg' (12234 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193334/3884471167.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb = torch.tensor(xb, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicting location 'left_leg' (12234 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193334/3884471167.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb = torch.tensor(xb, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to work/result.csv\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bde7f9f4206ae8d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "551a71d09c8a1743"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
