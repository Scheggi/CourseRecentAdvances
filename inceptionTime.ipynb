{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"initial_id\",\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true,\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-28T22:32:34.300499Z\",\n",
    "     \"start_time\": \"2025-06-28T22:32:34.296252Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.optim as optim\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, f1_score\\n\",\n",
    "    \"from sympy import false\\n\",\n",
    "    \"from torch.utils.data import Dataset, DataLoader\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"import ast\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 11\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-28T22:32:34.685002Z\",\n",
    "     \"start_time\": \"2025-06-28T22:32:34.676429Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"label_map = {\\n\",\n",
    "    \"    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\\n\",\n",
    "    \"    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\\n\",\n",
    "    \"    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"test_subjects = [21]\\n\",\n",
    "    \"embedding_dim = 64\\n\",\n",
    "    \"num_classes = len(label_map.keys())\\n\",\n",
    "    \"sequence_length = 50\\n\",\n",
    "    \"input_channels = 3\\n\",\n",
    "    \"num_layers = 2\\n\",\n",
    "    \"hidden_size = 128\\n\",\n",
    "    \"learning_rate = 0.001\\n\",\n",
    "    \"num_epochs = 50\\n\",\n",
    "    \"batch_size = 32\\n\",\n",
    "    \"test_subjects = [21]\\n\",\n",
    "    \"loc = \\\"right_arm\\\"\\n\",\n",
    "    \"THRESHOLD: float = 0.25\\n\",\n",
    "    \"DEFAULT_CLASS: int = 0\\n\",\n",
    "    \"device =  'cpu'# torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"RAW_DIR   = Path('data')\\n\",\n",
    "    \"TRAIN_DIR = RAW_DIR / 'dataset_without_null'\\n\",\n",
    "    \"TEST_CSV  = RAW_DIR / 'test.csv'                   # public test\\n\",\n",
    "    \"LOCATION_IDS = [\\\"right_arm\\\", \\\"left_arm\\\", \\\"right_leg\\\", \\\"left_leg\\\"]\\n\",\n",
    "    \"WORK_DIR = Path('inception')\"\n",
    "   ],\n",
    "   \"id\": \"43beaf61431c0e23\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 12\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"class HARDataset(Dataset):\\n\",\n",
    "    \"    def __init__(self, data):\\n\",\n",
    "    \"        self.data = data\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        return len(self.data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __getitem__(self, idx):\\n\",\n",
    "    \"        sample = self.data[idx]\\n\",\n",
    "    \"        x = np.array(ast.literal_eval(sample['x_axis']))\\n\",\n",
    "    \"        y = np.array(ast.literal_eval(sample['y_axis']))\\n\",\n",
    "    \"        z = np.array(ast.literal_eval(sample['z_axis']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"        data = np.vstack((x, y, z)).T\\n\",\n",
    "    \"        label = int(sample['label'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return torch.FloatTensor(data), torch.LongTensor([label])\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_and_split_data(data_dir: Path, loc: str):\\n\",\n",
    "    \"    csv_path = data_dir / f\\\"{loc}_windows.csv\\\"\\n\",\n",
    "    \"    df = pd.read_csv(csv_path)\\n\",\n",
    "    \"    train_data = df[~df['sbj_id'].isin(test_subjects)]\\n\",\n",
    "    \"    test_data = df[df['sbj_id'].isin(test_subjects)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    train_dataset = HARDataset(train_data.to_dict('records'))\\n\",\n",
    "    \"    test_dataset = HARDataset(test_data.to_dict('records'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return train_dataset, test_dataset\"\n",
    "   ],\n",
    "   \"id\": \"b556aacf1b0882a5\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"train_dataset, test_dataset = load_and_split_data(TRAIN_DIR,LOCATION_IDS[0])\\n\",\n",
    "    \"train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\n\",\n",
    "    \"test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_loader\"\n",
    "   ],\n",
    "   \"id\": \"26ad005a27f1a666\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:17.515745Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:17.507321Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import ast, numpy as np, pandas as pd, torch\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from torch.utils.data import Dataset, DataLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"WINDOW_LEN = 50          # 50 Hz × 1 s\\n\",\n",
    "    \"N_AXES      = 3          # x, y, z\\n\",\n",
    "    \"N_CLASSES   = 19         # change if your label space differs\\n\",\n",
    "    \"\\n\",\n",
    "    \"def str2vec(s):\\n\",\n",
    "    \"    return np.array(ast.literal_eval(s), dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"class WearDataset(Dataset):\\n\",\n",
    "    \"    def __init__(self, df, scaler=None, fit=False):\\n\",\n",
    "    \"        x = np.stack([\\n\",\n",
    "    \"            np.stack([str2vec(df[c].iloc[i]) for c in ('x_axis','y_axis','z_axis')], axis=1)\\n\",\n",
    "    \"            for i in range(len(df))\\n\",\n",
    "    \"        ])                # shape: (N, 50, 3)\\n\",\n",
    "    \"        self.X = x.reshape(len(df), -1)  # flatten for scaler\\n\",\n",
    "    \"        if scaler is None:\\n\",\n",
    "    \"            scaler = StandardScaler()\\n\",\n",
    "    \"        if fit:\\n\",\n",
    "    \"            scaler.fit(self.X)\\n\",\n",
    "    \"        self.X = scaler.transform(self.X).reshape(-1, WINDOW_LEN, N_AXES)\\n\",\n",
    "    \"        self.X = torch.tensor(self.X).permute(0,2,1)  # (N,3,50) for 1‑D CNN\\n\",\n",
    "    \"        self.y = None if 'label' not in df else torch.tensor(df['label'].values, dtype=torch.long)\\n\",\n",
    "    \"        self.scaler = scaler\\n\",\n",
    "    \"    def __len__(self): return len(self.X)\\n\",\n",
    "    \"    def __getitem__(self, idx):\\n\",\n",
    "    \"        if self.y is None:\\n\",\n",
    "    \"            return self.X[idx]\\n\",\n",
    "    \"        return self.X[idx], self.y[idx]\\n\",\n",
    "    \"\\n\",\n",
    "    \"def predict_with_default(softmatrix, threshold=0.25, default_class=0):\\n\",\n",
    "    \"    max_conf = softmatrix.max(axis=1)\\n\",\n",
    "    \"    preds = softmatrix.argmax(axis=1)\\n\",\n",
    "    \"    return np.where(max_conf < threshold, default_class, preds)\\n\"\n",
    "   ],\n",
    "   \"id\": \"73eb63b43b942b42\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 30\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:17.813004Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:17.807886Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import torch, torch.nn as nn\\n\",\n",
    "    \"class InceptionBlock(nn.Module):\\n\",\n",
    "    \"    def __init__(self, c_in, c_out, kernels=(10,20,40)):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.bottleneck = nn.Conv1d(c_in, c_out//4, 1, bias=False,padding='same')\\n\",\n",
    "    \"        self.convlist   = nn.ModuleList(\\n\",\n",
    "    \"            [nn.Conv1d(c_out//4, c_out//4, k, padding='same', bias=False)\\n\",\n",
    "    \"             for k in kernels])\\n\",\n",
    "    \"        self.pool_conv  = nn.Sequential(\\n\",\n",
    "    \"            nn.MaxPool1d(3, stride=1, padding=1),\\n\",\n",
    "    \"            nn.Conv1d(c_in, c_out//4, 1, bias=False, padding='same')\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        self.bn = nn.BatchNorm1d(c_out)\\n\",\n",
    "    \"        self.relu = nn.ReLU()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self,x):\\n\",\n",
    "    \"        bott = self.bottleneck(x)\\n\",\n",
    "    \"        conv_outs = [conv(bott) for conv in self.convlist]\\n\",\n",
    "    \"        pool_out = self.pool_conv(x)\\n\",\n",
    "    \"        y = torch.cat(conv_outs + [pool_out], dim=1)\\n\",\n",
    "    \"        return self.relu(self.bn(y))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class InceptionTime(nn.Module):\\n\",\n",
    "    \"    def __init__(self, n_channels=3, n_classes=19, n_blocks=3, c_out=32):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        blocks = []\\n\",\n",
    "    \"        c_in = n_channels\\n\",\n",
    "    \"        for _ in range(n_blocks):\\n\",\n",
    "    \"            blocks.append(InceptionBlock(c_in, c_out))\\n\",\n",
    "    \"            c_in = c_out\\n\",\n",
    "    \"        # feature extractor: Inception layers + global average pooling\\n\",\n",
    "    \"        self.feature = nn.Sequential(\\n\",\n",
    "    \"            *blocks,\\n\",\n",
    "    \"            nn.AdaptiveAvgPool1d(output_size=1)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        # final classifier\\n\",\n",
    "    \"        self.classifier = nn.Linear(c_out, n_classes)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        # x: (batch, channels, length)\\n\",\n",
    "    \"        features = self.feature(x)         # (batch, c_out, 1)\\n\",\n",
    "    \"        features = features.squeeze(-1)    # (batch, c_out)\\n\",\n",
    "    \"        return self.classifier(features)   # (batch, n_classes)\\n\"\n",
    "   ],\n",
    "   \"id\": \"15f56765a50a3aed\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 31\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:18.361433Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:18.354156Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd, torch, torch.nn as nn, torch.optim as optim\\n\",\n",
    "    \"from sklearn.model_selection import LeaveOneGroupOut\\n\",\n",
    "    \"import os, json, numpy as np, argparse\\n\",\n",
    "    \"\\n\",\n",
    "    \"def train_modality(csv_path, model_dir, batch=128, lr=1e-3, epochs=60):\\n\",\n",
    "    \"    df = pd.read_csv(csv_path)\\n\",\n",
    "    \"    groups = df['sbj_id'].values\\n\",\n",
    "    \"    logo = LeaveOneGroupOut()\\n\",\n",
    "    \"    oof_soft = np.zeros((len(df), 19), dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for fold,(tr,va) in enumerate(logo.split(df, groups=groups)):\\n\",\n",
    "    \"        tr_ds = WearDataset(df.iloc[tr], fit=True)\\n\",\n",
    "    \"        va_ds = WearDataset(df.iloc[va], scaler=tr_ds.scaler)\\n\",\n",
    "    \"        tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=batch, shuffle=True)\\n\",\n",
    "    \"        va_dl = torch.utils.data.DataLoader(va_ds, batch_size=batch, shuffle=False)\\n\",\n",
    "    \"        model = InceptionTime().cuda()\\n\",\n",
    "    \"        print(\\\"Total params:\\\", sum(p.numel() for p in model.parameters()))\\n\",\n",
    "    \"        opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\\n\",\n",
    "    \"        crit  = nn.CrossEntropyLoss()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        best_f1, patience = 0, 10\\n\",\n",
    "    \"        for ep in range(epochs):\\n\",\n",
    "    \"            model.train()\\n\",\n",
    "    \"            for xb,yb in tr_dl:\\n\",\n",
    "    \"                xb,yb = xb.cuda(), yb.cuda()\\n\",\n",
    "    \"                opt.zero_grad()\\n\",\n",
    "    \"                loss = crit(model(xb), yb)\\n\",\n",
    "    \"                loss.backward()\\n\",\n",
    "    \"                opt.step()\\n\",\n",
    "    \"            model.eval()\\n\",\n",
    "    \"            with torch.no_grad():\\n\",\n",
    "    \"                probs, targs = [], []\\n\",\n",
    "    \"                for xb,yb in va_dl:\\n\",\n",
    "    \"                    p = torch.softmax(model(xb.cuda()), -1).cpu()\\n\",\n",
    "    \"                    probs.append(p); targs.append(yb)\\n\",\n",
    "    \"                probs = torch.cat(probs); targs = torch.cat(targs)\\n\",\n",
    "    \"                preds = predict_with_default(probs.numpy())\\n\",\n",
    "    \"                f1 = f1_score(\\n\",\n",
    "    \"                        targs, preds, average='macro')\\n\",\n",
    "    \"            if f1 > best_f1:\\n\",\n",
    "    \"                best_f1 = f1; patience = 10\\n\",\n",
    "    \"                torch.save({'scaler_mean': tr_ds.scaler.mean_,\\n\",\n",
    "    \"                            'scaler_scale': tr_ds.scaler.scale_,\\n\",\n",
    "    \"                            'state_dict': model.state_dict()},\\n\",\n",
    "    \"                           f'{model_dir}/fold{fold}.pt')\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                patience -= 1\\n\",\n",
    "    \"                if patience==0: break\\n\",\n",
    "    \"        print(f'fold {fold} best‑macro‑F1: {best_f1:.4f}')\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # fill out‑of‑fold matrix for debugging / ensemble later\\n\",\n",
    "    \"        model.load_state_dict(torch.load(f'{model_dir}/fold{fold}.pt', weights_only=false)['state_dict'])\\n\",\n",
    "    \"        model.eval()\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            va_probs = []\\n\",\n",
    "    \"            for xb,_ in va_dl:\\n\",\n",
    "    \"                va_probs.append(torch.softmax(model(xb.cuda()),-1).cpu())\\n\",\n",
    "    \"        oof_soft[va] = torch.cat(va_probs).numpy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    np.save(f'{model_dir}/oof_probs.npy', oof_soft)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ],\n",
    "   \"id\": \"803e4f1a92c6f99a\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 32\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:18.733881Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:18.731761Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"\",\n",
    "   \"id\": \"ce8e1db0e355abd9\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:19.079385Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:19.076138Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def predict_with_default(softmatrix: np.ndarray, threshold: float = 0.25, default_class: int = 0) -> np.ndarray:\\n\",\n",
    "    \"    max_conf = softmatrix.max(axis=1)\\n\",\n",
    "    \"    preds = softmatrix.argmax(axis=1)\\n\",\n",
    "    \"    return np.where(max_conf < threshold, default_class, preds)\\n\"\n",
    "   ],\n",
    "   \"id\": \"e933526a93b81fc\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 33\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-06-29T00:16:55.224379Z\",\n",
    "     \"start_time\": \"2025-06-29T00:16:19.807007Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"os.makedirs(WORK_DIR, exist_ok=True)\\n\",\n",
    "    \"for entry in LOCATION_IDS[:1]:\\n\",\n",
    "    \"    train_modality(TRAIN_DIR / f'{entry}_windows.csv', WORK_DIR)\"\n",
    "   ],\n",
    "   \"id\": \"b70421264510fd21\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"ename\": \"RuntimeError\",\n",
    "     \"evalue\": \"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001B[31m---------------------------------------------------------------------------\\u001B[39m\",\n",
    "      \"\\u001B[31mRuntimeError\\u001B[39m                              Traceback (most recent call last)\",\n",
    "      \"\\u001B[36mCell\\u001B[39m\\u001B[36m \\u001B[39m\\u001B[32mIn[34]\\u001B[39m\\u001B[32m, line 3\\u001B[39m\\n\\u001B[32m      1\\u001B[39m os.makedirs(WORK_DIR, exist_ok=\\u001B[38;5;28;01mTrue\\u001B[39;00m)\\n\\u001B[32m      2\\u001B[39m \\u001B[38;5;28;01mfor\\u001B[39;00m entry \\u001B[38;5;129;01min\\u001B[39;00m LOCATION_IDS[:\\u001B[32m1\\u001B[39m]:\\n\\u001B[32m----> \\u001B[39m\\u001B[32m3\\u001B[39m     \\u001B[43mtrain_modality\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mTRAIN_DIR\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[43m/\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[33;43mf\\u001B[39;49m\\u001B[33;43m'\\u001B[39;49m\\u001B[38;5;132;43;01m{\\u001B[39;49;00m\\u001B[43mentry\\u001B[49m\\u001B[38;5;132;43;01m}\\u001B[39;49;00m\\u001B[33;43m_windows.csv\\u001B[39;49m\\u001B[33;43m'\\u001B[39;49m\\u001B[43m,\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[43mWORK_DIR\\u001B[49m\\u001B[43m)\\u001B[49m\\n\",\n",
    "      \"\\u001B[36mCell\\u001B[39m\\u001B[36m \\u001B[39m\\u001B[32mIn[32]\\u001B[39m\\u001B[32m, line 16\\u001B[39m, in \\u001B[36mtrain_modality\\u001B[39m\\u001B[34m(csv_path, model_dir, batch, lr, epochs)\\u001B[39m\\n\\u001B[32m     14\\u001B[39m tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=batch, shuffle=\\u001B[38;5;28;01mTrue\\u001B[39;00m)\\n\\u001B[32m     15\\u001B[39m va_dl = torch.utils.data.DataLoader(va_ds, batch_size=batch, shuffle=\\u001B[38;5;28;01mFalse\\u001B[39;00m)\\n\\u001B[32m---> \\u001B[39m\\u001B[32m16\\u001B[39m model = \\u001B[43mInceptionTime\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43mcuda\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m     17\\u001B[39m \\u001B[38;5;28mprint\\u001B[39m(\\u001B[33m\\\"\\u001B[39m\\u001B[33mTotal params:\\u001B[39m\\u001B[33m\\\"\\u001B[39m, \\u001B[38;5;28msum\\u001B[39m(p.numel() \\u001B[38;5;28;01mfor\\u001B[39;00m p \\u001B[38;5;129;01min\\u001B[39;00m model.parameters()))\\n\\u001B[32m     18\\u001B[39m opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=\\u001B[32m1e-4\\u001B[39m)\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\\u001B[39m, in \\u001B[36mModule.cuda\\u001B[39m\\u001B[34m(self, device)\\u001B[39m\\n\\u001B[32m   1048\\u001B[39m \\u001B[38;5;28;01mdef\\u001B[39;00m\\u001B[38;5;250m \\u001B[39m\\u001B[34mcuda\\u001B[39m(\\u001B[38;5;28mself\\u001B[39m: T, device: Optional[Union[\\u001B[38;5;28mint\\u001B[39m, device]] = \\u001B[38;5;28;01mNone\\u001B[39;00m) -> T:\\n\\u001B[32m   1049\\u001B[39m \\u001B[38;5;250m    \\u001B[39m\\u001B[33mr\\u001B[39m\\u001B[33;03m\\\"\\\"\\\"Move all model parameters and buffers to the GPU.\\u001B[39;00m\\n\\u001B[32m   1050\\u001B[39m \\n\\u001B[32m   1051\\u001B[39m \\u001B[33;03m    This also makes associated parameters and buffers different objects. So\\u001B[39;00m\\n\\u001B[32m   (...)\\u001B[39m\\u001B[32m   1063\\u001B[39m \\u001B[33;03m        Module: self\\u001B[39;00m\\n\\u001B[32m   1064\\u001B[39m \\u001B[33;03m    \\\"\\\"\\\"\\u001B[39;00m\\n\\u001B[32m-> \\u001B[39m\\u001B[32m1065\\u001B[39m     \\u001B[38;5;28;01mreturn\\u001B[39;00m \\u001B[38;5;28;43mself\\u001B[39;49m\\u001B[43m.\\u001B[49m\\u001B[43m_apply\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[38;5;28;43;01mlambda\\u001B[39;49;00m\\u001B[43m \\u001B[49m\\u001B[43mt\\u001B[49m\\u001B[43m:\\u001B[49m\\u001B[43m \\u001B[49m\\u001B[43mt\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43mcuda\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mdevice\\u001B[49m\\u001B[43m)\\u001B[49m\\u001B[43m)\\u001B[49m\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\\u001B[39m, in \\u001B[36mModule._apply\\u001B[39m\\u001B[34m(self, fn, recurse)\\u001B[39m\\n\\u001B[32m    913\\u001B[39m \\u001B[38;5;28;01mif\\u001B[39;00m recurse:\\n\\u001B[32m    914\\u001B[39m     \\u001B[38;5;28;01mfor\\u001B[39;00m module \\u001B[38;5;129;01min\\u001B[39;00m \\u001B[38;5;28mself\\u001B[39m.children():\\n\\u001B[32m--> \\u001B[39m\\u001B[32m915\\u001B[39m         \\u001B[43mmodule\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43m_apply\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mfn\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m    917\\u001B[39m \\u001B[38;5;28;01mdef\\u001B[39;00m\\u001B[38;5;250m \\u001B[39m\\u001B[34mcompute_should_use_set_data\\u001B[39m(tensor, tensor_applied):\\n\\u001B[32m    918\\u001B[39m     \\u001B[38;5;28;01mif\\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\\n\\u001B[32m    919\\u001B[39m         \\u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\\u001B[39;00m\\n\\u001B[32m    920\\u001B[39m         \\u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\\u001B[39;00m\\n\\u001B[32m   (...)\\u001B[39m\\u001B[32m    925\\u001B[39m         \\u001B[38;5;66;03m# global flag to let the user control whether they want the future\\u001B[39;00m\\n\\u001B[32m    926\\u001B[39m         \\u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\\u001B[39;00m\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\\u001B[39m, in \\u001B[36mModule._apply\\u001B[39m\\u001B[34m(self, fn, recurse)\\u001B[39m\\n\\u001B[32m    913\\u001B[39m \\u001B[38;5;28;01mif\\u001B[39;00m recurse:\\n\\u001B[32m    914\\u001B[39m     \\u001B[38;5;28;01mfor\\u001B[39;00m module \\u001B[38;5;129;01min\\u001B[39;00m \\u001B[38;5;28mself\\u001B[39m.children():\\n\\u001B[32m--> \\u001B[39m\\u001B[32m915\\u001B[39m         \\u001B[43mmodule\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43m_apply\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mfn\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m    917\\u001B[39m \\u001B[38;5;28;01mdef\\u001B[39;00m\\u001B[38;5;250m \\u001B[39m\\u001B[34mcompute_should_use_set_data\\u001B[39m(tensor, tensor_applied):\\n\\u001B[32m    918\\u001B[39m     \\u001B[38;5;28;01mif\\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\\n\\u001B[32m    919\\u001B[39m         \\u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\\u001B[39;00m\\n\\u001B[32m    920\\u001B[39m         \\u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\\u001B[39;00m\\n\\u001B[32m   (...)\\u001B[39m\\u001B[32m    925\\u001B[39m         \\u001B[38;5;66;03m# global flag to let the user control whether they want the future\\u001B[39;00m\\n\\u001B[32m    926\\u001B[39m         \\u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\\u001B[39;00m\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\\u001B[39m, in \\u001B[36mModule._apply\\u001B[39m\\u001B[34m(self, fn, recurse)\\u001B[39m\\n\\u001B[32m    913\\u001B[39m \\u001B[38;5;28;01mif\\u001B[39;00m recurse:\\n\\u001B[32m    914\\u001B[39m     \\u001B[38;5;28;01mfor\\u001B[39;00m module \\u001B[38;5;129;01min\\u001B[39;00m \\u001B[38;5;28mself\\u001B[39m.children():\\n\\u001B[32m--> \\u001B[39m\\u001B[32m915\\u001B[39m         \\u001B[43mmodule\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43m_apply\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mfn\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m    917\\u001B[39m \\u001B[38;5;28;01mdef\\u001B[39;00m\\u001B[38;5;250m \\u001B[39m\\u001B[34mcompute_should_use_set_data\\u001B[39m(tensor, tensor_applied):\\n\\u001B[32m    918\\u001B[39m     \\u001B[38;5;28;01mif\\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\\n\\u001B[32m    919\\u001B[39m         \\u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\\u001B[39;00m\\n\\u001B[32m    920\\u001B[39m         \\u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\\u001B[39;00m\\n\\u001B[32m   (...)\\u001B[39m\\u001B[32m    925\\u001B[39m         \\u001B[38;5;66;03m# global flag to let the user control whether they want the future\\u001B[39;00m\\n\\u001B[32m    926\\u001B[39m         \\u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\\u001B[39;00m\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:942\\u001B[39m, in \\u001B[36mModule._apply\\u001B[39m\\u001B[34m(self, fn, recurse)\\u001B[39m\\n\\u001B[32m    938\\u001B[39m \\u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\\u001B[39;00m\\n\\u001B[32m    939\\u001B[39m \\u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\\u001B[39;00m\\n\\u001B[32m    940\\u001B[39m \\u001B[38;5;66;03m# `with torch.no_grad():`\\u001B[39;00m\\n\\u001B[32m    941\\u001B[39m \\u001B[38;5;28;01mwith\\u001B[39;00m torch.no_grad():\\n\\u001B[32m--> \\u001B[39m\\u001B[32m942\\u001B[39m     param_applied = \\u001B[43mfn\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mparam\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m    943\\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\\n\\u001B[32m    945\\u001B[39m \\u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\\u001B[39;00m\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\\u001B[39m, in \\u001B[36mModule.cuda.<locals>.<lambda>\\u001B[39m\\u001B[34m(t)\\u001B[39m\\n\\u001B[32m   1048\\u001B[39m \\u001B[38;5;28;01mdef\\u001B[39;00m\\u001B[38;5;250m \\u001B[39m\\u001B[34mcuda\\u001B[39m(\\u001B[38;5;28mself\\u001B[39m: T, device: Optional[Union[\\u001B[38;5;28mint\\u001B[39m, device]] = \\u001B[38;5;28;01mNone\\u001B[39;00m) -> T:\\n\\u001B[32m   1049\\u001B[39m \\u001B[38;5;250m    \\u001B[39m\\u001B[33mr\\u001B[39m\\u001B[33;03m\\\"\\\"\\\"Move all model parameters and buffers to the GPU.\\u001B[39;00m\\n\\u001B[32m   1050\\u001B[39m \\n\\u001B[32m   1051\\u001B[39m \\u001B[33;03m    This also makes associated parameters and buffers different objects. So\\u001B[39;00m\\n\\u001B[32m   (...)\\u001B[39m\\u001B[32m   1063\\u001B[39m \\u001B[33;03m        Module: self\\u001B[39;00m\\n\\u001B[32m   1064\\u001B[39m \\u001B[33;03m    \\\"\\\"\\\"\\u001B[39;00m\\n\\u001B[32m-> \\u001B[39m\\u001B[32m1065\\u001B[39m     \\u001B[38;5;28;01mreturn\\u001B[39;00m \\u001B[38;5;28mself\\u001B[39m._apply(\\u001B[38;5;28;01mlambda\\u001B[39;00m t: \\u001B[43mt\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43mcuda\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43mdevice\\u001B[49m\\u001B[43m)\\u001B[49m)\\n\",\n",
    "      \"\\u001B[36mFile \\u001B[39m\\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:372\\u001B[39m, in \\u001B[36m_lazy_init\\u001B[39m\\u001B[34m()\\u001B[39m\\n\\u001B[32m    370\\u001B[39m \\u001B[38;5;28;01mif\\u001B[39;00m \\u001B[33m\\\"\\u001B[39m\\u001B[33mCUDA_MODULE_LOADING\\u001B[39m\\u001B[33m\\\"\\u001B[39m \\u001B[38;5;129;01mnot\\u001B[39;00m \\u001B[38;5;129;01min\\u001B[39;00m os.environ:\\n\\u001B[32m    371\\u001B[39m     os.environ[\\u001B[33m\\\"\\u001B[39m\\u001B[33mCUDA_MODULE_LOADING\\u001B[39m\\u001B[33m\\\"\\u001B[39m] = \\u001B[33m\\\"\\u001B[39m\\u001B[33mLAZY\\u001B[39m\\u001B[33m\\\"\\u001B[39m\\n\\u001B[32m--> \\u001B[39m\\u001B[32m372\\u001B[39m \\u001B[43mtorch\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43m_C\\u001B[49m\\u001B[43m.\\u001B[49m\\u001B[43m_cuda_init\\u001B[49m\\u001B[43m(\\u001B[49m\\u001B[43m)\\u001B[49m\\n\\u001B[32m    373\\u001B[39m \\u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\\u001B[39;00m\\n\\u001B[32m    374\\u001B[39m \\u001B[38;5;66;03m# we need to just return without initializing in that case.\\u001B[39;00m\\n\\u001B[32m    375\\u001B[39m \\u001B[38;5;66;03m# However, we must not let any *other* threads in!\\u001B[39;00m\\n\\u001B[32m    376\\u001B[39m _tls.is_initializing = \\u001B[38;5;28;01mTrue\\u001B[39;00m\\n\",\n",
    "      \"\\u001B[31mRuntimeError\\u001B[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 34\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": \"\",\n",
    "   \"id\": \"1320205345cbee12\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "a0355b8d079a8875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d65a31bf4fe44d7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
