{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T19:20:01.079829Z",
     "start_time": "2025-06-21T19:20:01.075018Z"
    }
   },
   "source": [
    "import os, gc, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "try:\n",
    "    from sktime.classification.kernel_based import RocketClassifier\n",
    "except Exception:\n",
    "    try:\n",
    "        from sktime.classification.kernel_based.rocket import RocketClassifier\n",
    "    except Exception:\n",
    "        raise ImportError(\n",
    "            \"RocketClassifier konnte nicht importiert werden. \"\n",
    "            \"Bitte installiere scikit-learn==1.2.2 und sktime==0.18.1\"\n",
    "        )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:20:44.996863Z",
     "start_time": "2025-06-21T19:20:44.989994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WIN = 128   # ~2.56 s @50 Hz\n",
    "STEP = 64   # 50 % overlap\n",
    "ROCKET_KERNELS = 10000\n",
    "data_dir = Path('data')\n",
    "train_dir = data_dir / 'train'\n",
    "meta_file = data_dir / 'meta_data.txt'\n",
    "test_file = data_dir/'test.csv'\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "num_classes = len(label_map)\n",
    "C = 3\n",
    "crit = nn.CrossEntropyLoss()\n",
    "print(len(label_map))"
   ],
   "id": "2fca800712fe05f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:22:03.251490Z",
     "start_time": "2025-06-21T19:21:50.863656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frames = []\n",
    "for f in sorted(train_dir.glob('sbj_*.csv')):\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df['subject'] = df['sbj_id'].astype(str)          # Text‑ID\n",
    "    frames.append(df)\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "print('Shape raw:', raw.shape)\n",
    "\n",
    "raw['label_code'] = raw['label'].map(label_map)\n",
    "raw = raw.dropna(subset=['label_code']).reset_index(drop=True)\n",
    "raw['label_code'] = raw['label_code'].astype(int)\n",
    "\n",
    "sensor_cols = [c for c in raw.columns if c.endswith(('_x','_y','_z'))]\n",
    "locs = sorted({c[:-2] for c in sensor_cols})          # alphabetische Liste aller Locations\n",
    "ax_order = ['_x','_y','_z']\n",
    "loc_axes_cols = {loc:[f'{loc}{a}' for a in ax_order] for loc in locs}\n",
    "print('Locations:', locs)\n",
    "\n",
    "before = len(raw)\n",
    "raw = raw.dropna(subset=sensor_cols).reset_index(drop=True)\n",
    "print(f'Remove NaN‑rows in sensors: {before-len(raw)} rows dropped')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "raw[sensor_cols] = scaler.fit_transform(raw[sensor_cols])\n",
    "\n",
    "raw['time_idx'] = raw.groupby('subject').cumcount()\n",
    "\n",
    "dfs_long = []"
   ],
   "id": "f213c60bef1f828a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape raw: (3466400, 15)\n",
      "Locations: ['left_arm_acc', 'left_leg_acc', 'right_arm_acc', 'right_leg_acc']\n",
      "Remove NaN‑rows in sensors: 34274 rows dropped\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:22:26.473698Z",
     "start_time": "2025-06-21T19:22:26.467076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepConvLSTM_Single(nn.Module):\n",
    "    def __init__(self, in_ch=3, classes=num_classes, hidden=128, conv_ch=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, conv_ch, 5, padding=2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_ch, conv_ch, 5, padding=2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_ch, conv_ch, 5, padding=2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_ch, conv_ch, 5, padding=2), nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(conv_ch, hidden, num_layers=2, batch_first=True, dropout=dropout)\n",
    "        self.fc   = nn.Linear(hidden, classes)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        out,_ = self.lstm(x)\n",
    "        return self.fc(out[:,-1])\n",
    "\n"
   ],
   "id": "3990d87cd68d4236",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:22:34.779596Z",
     "start_time": "2025-06-21T19:22:32.734991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for loc in locs:\n",
    "    cols = loc_axes_cols[loc]\n",
    "    df_loc = raw[['subject','time_idx','label_code'] + cols].copy()\n",
    "    df_loc = df_loc.rename(columns={cols[0]: 'x_axis', cols[1]: 'y_axis', cols[2]: 'z_axis'})\n",
    "    df_loc['sensor_location'] = loc\n",
    "    df_loc['sbj_id'] = df_loc['subject']\n",
    "    df_loc['id'] = df_loc['time_idx']\n",
    "    dfs_long.append(df_loc[['id','sbj_id','sensor_location','x_axis','y_axis','z_axis','label_code']])\n",
    "long_df = pd.concat(dfs_long, ignore_index=True)\n",
    "print('Langformat Trainingsdaten:', long_df.shape)\n",
    "print('Beispiel:', long_df.head())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "raw[sensor_cols] = scaler.fit_transform(raw[sensor_cols])"
   ],
   "id": "e8efa261376a185b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langformat Trainingsdaten: (8219056, 7)\n",
      "Beispiel:    id sbj_id sensor_location    x_axis    y_axis    z_axis  label_code\n",
      "0   0      0    left_arm_acc -0.515422  0.331682  0.323862           1\n",
      "1   1      0    left_arm_acc -0.478973  0.336796  0.330691           1\n",
      "2   2      0    left_arm_acc -0.411302  0.327018  0.366185           1\n",
      "3   3      0    left_arm_acc -0.349172  0.311192  0.397661           1\n",
      "4   4      0    left_arm_acc -0.295446  0.247657  0.381219           1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:23:03.743129Z",
     "start_time": "2025-06-21T19:22:42.321148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_all, y_all, subj_all, loc_all = [], [], [], []\n",
    "for subj_id, sub_df in tqdm(raw.groupby('subject')):\n",
    "    for loc in locs:\n",
    "        cols = loc_axes_cols[loc]\n",
    "        if not all(c in sub_df.columns for c in cols):\n",
    "            continue\n",
    "        sub_loc = sub_df.dropna(subset=cols).reset_index(drop=True)\n",
    "        if len(sub_loc) < WIN:\n",
    "            continue\n",
    "        data = sub_loc[cols].values  # (T_loc, 3)\n",
    "        # Sliding Windows\n",
    "        for s in range(0, len(data) - WIN + 1, STEP):\n",
    "            win = data[s:s+WIN]\n",
    "            X_all.append(win)\n",
    "            y_all.append(sub_loc['label_code'].iloc[s:s+WIN].mode()[0])\n",
    "            subj_all.append(subj_id)\n",
    "            loc_all.append(locs.index(loc))\n",
    "# Arrays erzeugen\n",
    "X_all = np.stack(X_all)\n",
    "y_all = np.array(y_all)\n",
    "loc_all = np.array(loc_all)\n",
    "print('Gesamt‑Windows:', X_all.shape)"
   ],
   "id": "f53209ab68aa4c3b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:20<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt‑Windows: (128292, 128, 3)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:23:03.905013Z",
     "start_time": "2025-06-21T19:23:03.896937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "class LocationWindowDataset(Dataset):\n",
    "    def __init__(self, df_loc, win, step):\n",
    "        self.df = df_loc.reset_index(drop=True)\n",
    "        self.win = win\n",
    "        self.step = step\n",
    "        # Berechne Startpositionen\n",
    "        self.starts = list(range(0, len(self.df) - win + 1, step))\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.starts[idx]\n",
    "        window = self.df.iloc[s:s+self.win]\n",
    "        # Channels-first Array\n",
    "        x = window[['x_axis','y_axis','z_axis']].values.T.astype(np.float32)\n",
    "        # Label per Window (Modus)\n",
    "        y = window['label_code'].mode()[0]\n",
    "        # Group for splitting\n",
    "        grp = window['sbj_id'].iloc[0]\n",
    "        return torch.from_numpy(x), int(y), grp"
   ],
   "id": "a431d20bfc0809ba",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T19:26:01.238980Z",
     "start_time": "2025-06-21T19:26:01.236098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sktime.transformations.panel.rocket import (\n",
    "    MiniRocket,\n",
    "    MiniRocketMultivariate,\n",
    "    MiniRocketMultivariateVariable,\n",
    ")"
   ],
   "id": "3e683b9b56ecc022",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-21T19:30:19.126923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv_models = {}\n",
    "rocket_models = {}\n",
    "conv_scores = {}\n",
    "rocket_scores = {}\n",
    "\n",
    "for loc in locs:\n",
    "    df_loc = long_df[long_df['sensor_location']==loc]\n",
    "    if df_loc.empty:\n",
    "        print(f\"Keine Daten für {loc}, überspringe.\")\n",
    "        continue\n",
    "\n",
    "    ds = LocationWindowDataset(df_loc, WIN, STEP)\n",
    "    n = len(ds)\n",
    "    groups = [ds[i][2] for i in range(n)]\n",
    "    y_all = [ds[i][1] for i in range(n)]\n",
    "\n",
    "    # Split via GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(list(range(n)), y_all, groups))\n",
    "    train_ds = Subset(ds, train_idx)\n",
    "    val_ds   = Subset(ds, val_idx)\n",
    "\n",
    "    # ConvLSTM-Loader\n",
    "    tr_ld = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    va_ld = DataLoader(val_ds,   batch_size=128)\n",
    "\n",
    "    model = DeepConvLSTM_Single(in_ch=3).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for ep in range(5):\n",
    "        model.train(); total_loss=0\n",
    "        for xb, yb, _ in tr_ld:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "        model.eval(); correct=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in va_ld:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb).argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "        acc = correct / len(val_ds)\n",
    "    conv_models[loc] = model\n",
    "    conv_scores[loc] = acc\n",
    "    print(f\"ConvLSTM {loc} VAL Acc: {acc:.3f}\")\n",
    "\n",
    "    MAX_ROCKET_WINDOWS = 20000\n",
    "    sel_tr_idx = np.random.RandomState(42).choice(train_idx, size=min(len(train_idx), MAX_ROCKET_WINDOWS), replace=False)\n",
    "    sel_vl_idx = np.random.RandomState(43).choice(val_idx,   size=min(len(val_idx),   5000),              replace=False)\n",
    "\n",
    "    def windows_to_panel(idxs):\n",
    "        plist, labels = [], []\n",
    "        for i in idxs:\n",
    "            x, y, _ = ds[i]\n",
    "            plist.append({\n",
    "                'x': pd.Series(x[0].cpu().numpy()),\n",
    "                'y': pd.Series(x[1].cpu().numpy()),\n",
    "                'z': pd.Series(x[2].cpu().numpy())\n",
    "            })\n",
    "            labels.append(y)\n",
    "        return pd.DataFrame(plist), np.array(labels)\n",
    "\n",
    "    Xp_tr, y_tr = windows_to_panel(train_idx)\n",
    "    Xp_vl, y_vl = windows_to_panel(val_idx)\n",
    "\n",
    "    rc = RocketClassifier(num_kernels=ROCKET_KERNELS, random_state=42)\n",
    "    rc.fit(Xp_tr, y_tr)\n",
    "    preds_vl = rc.predict(Xp_vl)\n",
    "    rocket_models[loc] = rc\n",
    "    rocket_scores[loc] = (preds_vl == y_vl).mean()\n",
    "    print(f\"Rocket {loc} VAL Acc: {rocket_scores[loc]:.3f}\")\n",
    "    del train_ds, val_ds, tr_ld, va_ld, model, rc, Xp_tr, Xp_vl, preds_vl, sel_tr_idx, sel_vl_idx\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ],
   "id": "b2f315e992902e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e8a825f6e91626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "for loc, m in rocket_models.items():\n",
    "    joblib.dump(m, f\"rocket_model_{loc}.pkl\")\n",
    "for loc, m in conv_models.items():\n",
    "    torch.save(m.state_dict(), f\"conv_model_{loc}.pt\")\n",
    "print(\"Modelle gespeichert.\")"
   ],
   "id": "3dd07c331443e93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "results = []\n",
    "for loc in locs:\n",
    "    rc = joblib.load(f\"rocket_model_{loc}.pkl\")\n",
    "    model = DeepConvLSTM_Single(in_ch=3)\n",
    "    model.load_state_dict(torch.load(f\"conv_model_{loc}.pt\", map_location=device))\n",
    "    model.to(device).eval()\n",
    "\n",
    "    df_loc = test_df[test_df['sensor_location']==loc].copy()\n",
    "    if df_loc.empty:\n",
    "        continue\n",
    "    seqs, ids = [], []\n",
    "    for sid, grp in df_loc.groupby('sbj_id'):\n",
    "        data = grp[['x_axis','y_axis','z_axis']].values\n",
    "        for s in range(0, len(data)-WIN+1, STEP):\n",
    "            seqs.append(data[s:s+WIN])\n",
    "            ids.append(grp['id'].iloc[s])\n",
    "    if not seqs:\n",
    "        continue\n",
    "    panel = [{'x':pd.Series(w[:,0]), 'y':pd.Series(w[:,1]), 'z':pd.Series(w[:,2])} for w in seqs]\n",
    "    Xp = pd.DataFrame(panel)\n",
    "    rocket_preds = rc.predict(Xp)\n",
    "    Xt = torch.tensor(np.stack(seqs).transpose(0,2,1), dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        conv_preds = model(Xt).argmax(1).cpu().numpy()\n",
    "    for idx, rp, cp in zip(ids, rocket_preds, conv_preds):\n",
    "        results.append({'id': idx, 'location': loc, 'rocket_pred': int(rp), 'conv_pred': int(cp)})\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"Test-Inferenz abgeschlossen. Zusammenfassung:\")\n",
    "display(res_df.head())\n",
    "res_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"Ergebnisse gespeichert: test_predictions.csv\")"
   ],
   "id": "a80caae2f85f3ff6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
