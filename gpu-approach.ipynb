{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T22:37:15.444069Z",
     "start_time": "2025-06-21T22:37:12.624945Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:37:15.461895Z",
     "start_time": "2025-06-21T22:37:15.458758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW = 50\n",
    "STRIDE = 25\n",
    "LOCATIONS = [\"right_arm\", \"left_arm\", \"right_leg\", \"left_leg\"]\n",
    "AXES = [\"x\", \"y\", \"z\"]"
   ],
   "id": "ee462701e0f48d2a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:47:19.839899Z",
     "start_time": "2025-06-21T22:47:19.835706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = Path('data')\n",
    "train_dir = data_dir / 'train'\n",
    "meta_file = data_dir / 'meta_data.txt'\n",
    "test_file = data_dir/'test.csv'\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "num_classes = len(label_map)\n",
    "C = 3\n",
    "crit = nn.CrossEntropyLoss()"
   ],
   "id": "e82032446b590bc4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:13:05.005139Z",
     "start_time": "2025-06-22T10:13:04.983076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_locations() -> list[str]:\n",
    "    files = sorted(train_dir.glob('sbj_*.csv'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No 'sbj_*.csv' files found in {data_dir}\")\n",
    "    sample = pd.read_csv(files[0], nrows=0)\n",
    "    locs = sorted({col.split('_acc_')[0] for col in sample.columns if '_acc_' in col})\n",
    "    if not locs:\n",
    "        raise ValueError(f\"No sensor columns found in {files[0]}\")\n",
    "    return locs\n",
    "    sample = pd.read_csv(next(data_dir.glob('sbj_*.csv')), nrows=0)\n",
    "    return sorted({col.split('_acc_')[0] for col in sample.columns if '_acc_' in col})\n",
    "\n",
    "def load_continuous_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def cols_for(location: str) -> List[str]:\n",
    "    return [f\"{location}_acc_{ax}\" for ax in AXES]\n",
    "\n",
    "\n",
    "def clean_location_df(df: pd.DataFrame, location: str, require_label: bool) -> pd.DataFrame:\n",
    "    \"\"\"Return only rows where the 3‑axis sensor values (and optionally label) are all non‑null.\"\"\"\n",
    "    cols = cols_for(location)\n",
    "    if require_label:\n",
    "        cols = cols + ['label']\n",
    "    return df.dropna(subset=cols)\n",
    "\n",
    "def segment_windows(\n",
    "    data: np.ndarray,\n",
    "    labels: np.ndarray | None,\n",
    "    window: int = WINDOW,\n",
    "    stride: int = STRIDE\n",
    ") -> Tuple[np.ndarray, np.ndarray | None]:\n",
    "    X_list, y_list = [], []\n",
    "    for start in range(0, len(data) - window + 1, stride):\n",
    "        seg = data[start:start + window]\n",
    "        if np.isnan(seg).any():\n",
    "            continue\n",
    "        X_list.append(seg)\n",
    "        if labels is not None:\n",
    "            lbl_seg = labels[start:start + window]\n",
    "            vals, counts = np.unique(lbl_seg, return_counts=True)\n",
    "            y_list.append(vals[counts.argmax()])\n",
    "    X = np.stack(X_list) if X_list else np.empty((0, window, 3), dtype=np.float32)\n",
    "    y = (np.array(y_list) if y_list else np.empty((0,))) if labels is not None else None\n",
    "    return X, y\n"
   ],
   "id": "878caa2bea004764",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T23:03:48.968213Z",
     "start_time": "2025-06-21T23:03:48.960716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class WearDataset(Dataset):\n",
    "    def __init__(self, windows: np.ndarray, labels: np.ndarray, scaler: StandardScaler):\n",
    "        self.X = scaler.transform(windows.reshape(-1, 3)).reshape(windows.shape)\n",
    "        self.y = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx]).permute(1, 0).contiguous()  # (C, T)\n",
    "        y = self.y[idx]\n",
    "        return x, y\n"
   ],
   "id": "264e488740b73e15",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T23:04:06.473262Z",
     "start_time": "2025-06-21T23:04:06.465523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self, n_channels: int = 3, n_classes: int = 19, seq_len: int = WINDOW):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,C,T)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.permute(0, 2, 1)  # (B,T,C')\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.classifier(h_n[-1])\n",
    "        return out\n"
   ],
   "id": "78852c1ee2c28fe8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:15:20.728112Z",
     "start_time": "2025-06-22T10:15:20.711138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, loader, criterion, optim, scaler):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for X, y in loader:\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        optim.zero_grad()\n",
    "        with autocast():\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        preds = out.argmax(1)\n",
    "        total += y.size(0)\n",
    "        correct += (preds == y).sum().item()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    scaler: GradScaler\n",
    ") -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for X, y in loader:\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (preds == y).sum().item()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "def run_location(train_dir: Path, test_path: Path, out_dir: Path, loc: str,\n",
    "                 epochs: int, batch: int):\n",
    "    train_files = sorted(train_dir.glob('sbj_*.csv'))\n",
    "    train_rows = []\n",
    "    for f in train_files:\n",
    "        df = clean_location_df(pd.read_csv(f), loc, require_label=True)\n",
    "        train_rows.append(df)\n",
    "    if not train_rows:\n",
    "        print(f\"[WARN] No data for {loc}, skipping.\")\n",
    "        return\n",
    "    train_df = pd.concat(train_rows, ignore_index=True)\n",
    "\n",
    "    test_df_raw = pd.read_csv(test_path)\n",
    "    test_df = clean_location_df(test_df_raw, loc, require_label=False)\n",
    "\n",
    "    X_train, y_train = segment_windows(train_df[cols_for(loc)].values.astype(np.float32),\n",
    "                                       train_df['label'].values)\n",
    "    if X_train.size == 0:\n",
    "        print(f\"[WARN] No windows for {loc}, skipping.\")\n",
    "        return\n",
    "\n",
    "    classes = sorted({*y_train})\n",
    "    lbl2idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_enc = np.array([lbl2idx[l] for l in y_train], dtype=np.int64)\n",
    "\n",
    "    # ---------- 5. Scaling ----------\n",
    "    scaler = StandardScaler().fit(X_train.reshape(-1, 3))\n",
    "    ds = WearDataset(X_train, y_enc, scaler)\n",
    "    loader = DataLoader(ds, batch_size=batch, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "    # ---------- 6. Model ----------\n",
    "    model = DeepConvLSTM(n_classes=len(classes)).cuda()\n",
    "    class_w = compute_class_weight('balanced', classes=np.unique(y_enc), y=y_enc)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_w, dtype=torch.float32).cuda())\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scaler_amp = GradScaler()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        loss, acc = train_loop(model, loader, criterion, optim, scaler_amp)\n",
    "        print(f\"[{loc}] Epoch {ep:02d}/{epochs} loss={loss:.4f} acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            ckpt = {\n",
    "                'model': model.state_dict(),\n",
    "                'scaler': scaler,\n",
    "                'label_map': lbl2idx,\n",
    "            }\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(ckpt, out_dir / f\"{loc}_best.pt\")\n",
    "            del ckpt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    del model, loader, ds, optim, criterion, scaler_amp\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            preds = logits.argmax(1)\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum/total, correct/total"
   ],
   "id": "66dd74a9666596db",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:15:24.366564Z",
     "start_time": "2025-06-22T10:15:24.356702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_file = next(train_dir.glob('sbj_*.csv'))\n",
    "print(first_file)\n",
    "all_locs = detect_locations()\n",
    "\n",
    "all_locs"
   ],
   "id": "a3580d7a6afd6b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/sbj_21.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left_arm', 'left_leg', 'right_arm', 'right_leg']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:15:47.472683Z",
     "start_time": "2025-06-22T10:15:38.055417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for loc in all_locs[:1]:\n",
    "    print(loc)\n",
    "    run_location(train_dir, test_file, \"result\", loc, 5, 256)\n"
   ],
   "id": "8bad1e9232585899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_arm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13746/3014088646.py:49: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_location_df(pd.read_csv(f), loc, require_label=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['left_arm_acc_x', 'left_arm_acc_y', 'left_arm_acc_z']",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[32m/tmp/ipykernel_13746/1126198777.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m loc \u001B[38;5;28;01min\u001B[39;00m all_locs[:\u001B[32m1\u001B[39m]:\n\u001B[32m      2\u001B[39m     print(loc)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     run_location(train_dir, test_file, \u001B[33m\"result\"\u001B[39m, loc, \u001B[32m5\u001B[39m, \u001B[32m256\u001B[39m)\n\u001B[32m      4\u001B[39m \n",
      "\u001B[32m/tmp/ipykernel_13746/3014088646.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(train_dir, test_path, out_dir, loc, epochs, batch)\u001B[39m\n\u001B[32m     53\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m     54\u001B[39m     train_df = pd.concat(train_rows, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     55\u001B[39m \n\u001B[32m     56\u001B[39m     test_df_raw = pd.read_csv(test_path)\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m     test_df = clean_location_df(test_df_raw, loc, require_label=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     58\u001B[39m \n\u001B[32m     59\u001B[39m     X_train, y_train = segment_windows(train_df[cols_for(loc)].values.astype(np.float32),\n\u001B[32m     60\u001B[39m                                        train_df[\u001B[33m'label'\u001B[39m].values)\n",
      "\u001B[32m/tmp/ipykernel_13746/965136763.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(df, location, require_label)\u001B[39m\n\u001B[32m     21\u001B[39m     \u001B[33m\"\"\"Return only rows where the 3‑axis sensor values (and optionally label) are all non‑null.\"\"\"\u001B[39m\n\u001B[32m     22\u001B[39m     cols = cols_for(location)\n\u001B[32m     23\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m require_label:\n\u001B[32m     24\u001B[39m         cols = cols + [\u001B[33m'label'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df.dropna(subset=cols)\n",
      "\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001B[39m\n\u001B[32m   6666\u001B[39m             ax = self._get_axis(agg_axis)\n\u001B[32m   6667\u001B[39m             indices = ax.get_indexer_for(subset)\n\u001B[32m   6668\u001B[39m             check = indices == -\u001B[32m1\u001B[39m\n\u001B[32m   6669\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m check.any():\n\u001B[32m-> \u001B[39m\u001B[32m6670\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m KeyError(np.array(subset)[check].tolist())\n\u001B[32m   6671\u001B[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001B[32m   6672\u001B[39m \n\u001B[32m   6673\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m thresh \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m lib.no_default:\n",
      "\u001B[31mKeyError\u001B[39m: ['left_arm_acc_x', 'left_arm_acc_y', 'left_arm_acc_z']"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler().fit(X.reshape(-1, 3))\n",
    "ds = WearDataset(X, y, scaler, augment=True)\n",
    "dl = DataLoader(ds, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "id": "9dff259eb8addf53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = DeepConvLSTM(n_classes=len(label_to_idx)).cuda()\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).cuda())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(args.epochs):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, dl, optimizer, criterion, scaler)\n",
    "    print(f'Epoch {epoch+1}/{args.epochs} - loss: {tr_loss:.4f} acc: {tr_acc:.4f}')\n",
    "    if tr_acc > best_acc:\n",
    "        best_acc = tr_acc\n",
    "        args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save({'model': model.state_dict(), 'scaler': scaler, 'label_map': label_to_idx},\n",
    "                   args.output_dir / f'{args.location}_best.pt')"
   ],
   "id": "356923b1cdae86c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
