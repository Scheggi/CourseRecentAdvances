{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T08:25:41.603871Z",
     "start_time": "2025-06-25T08:25:41.598024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, math, random, json, warnings, itertools, time, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from typing import Tuple, List\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "RAW_DIR   = Path('data')\n",
    "TRAIN_DIR = RAW_DIR / 'dataset_by_location'\n",
    "TEST_CSV  = RAW_DIR / 'test.csv'                   # public test\n",
    "LOCATION_IDS = [\"right_arm\", \"left_arm\", \"right_leg\", \"left_leg\"]\n",
    "WORK_DIR = Path('work3')\n",
    "WIN_SIZE  = 50\n",
    "STRIDE    = 25\n",
    "BATCH_SZ  = 256\n",
    "EPOCHS    = 20\n",
    "EMB_DIM   = 64\n",
    "LR        = 1e-3\n",
    "NUM_WORKERS = 4\n",
    "DEVICE    = 'cpu'#cuda' if torch.cuda.is_available() else 'cpu'\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "loc = \"right_arm\""
   ],
   "id": "c0e8c71361793e48",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T08:17:08.653022Z",
     "start_time": "2025-06-25T08:17:08.645373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepConvLSTMEmbedder(nn.Module):\n",
    "    \"\"\"3×T input → (embedding_dim, logits)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int,\n",
    "        embedding_dim: int = 128,\n",
    "        conv_channels: Tuple[int, int, int] = (32, 64, 96),\n",
    "        lstm_hidden: int = 128,\n",
    "        lstm_layers: int = 1,\n",
    "        dropout_p: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.feature_extractor = self._build_conv_stack(conv_channels, dropout_p)\n",
    "        self.temporal_model  = nn.LSTM(conv_channels[-1], lstm_hidden, lstm_layers,\n",
    "                                       batch_first=True)\n",
    "        self.embedding_head  = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, embedding_dim, bias=False),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.classifier      = nn.Linear(embedding_dim, n_classes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_conv_stack(channels: Tuple[int, int, int], dropout_p: float) -> nn.Sequential:\n",
    "        groups = [4, 8, 8]\n",
    "        kernels = [5, 7, 9]\n",
    "        layers: List[nn.Module] = []\n",
    "        in_c = 3\n",
    "        for out_c, g, k in zip(channels, groups, kernels):\n",
    "            layers += [\n",
    "                nn.Conv1d(in_c, out_c, kernel_size=k, padding=k//2),\n",
    "                nn.GroupNorm(g, out_c),\n",
    "                nn.SELU(inplace=True),\n",
    "            ]\n",
    "            if out_c != channels[-1]:\n",
    "                layers.append(nn.Dropout(dropout_p))\n",
    "            in_c = out_c\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # x: (B, 3, T)\n",
    "        feats = self.feature_extractor(x)          # (B, C, T)\n",
    "        feats = feats.permute(0, 2, 1)             # (B, T, C)\n",
    "        lstm_out, _ = self.temporal_model(feats)   # (B, T, H)\n",
    "        pooled = lstm_out.mean(dim=1)              # (B, H)\n",
    "        emb    = self.embedding_head(pooled)       # (B, D)\n",
    "        logits = self.classifier(emb)              # (B, n_cls)\n",
    "        return emb, logits"
   ],
   "id": "abfd088c47a2a9fb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T08:17:43.757505Z",
     "start_time": "2025-06-25T08:17:43.747556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepConvLSTMClassifier(nn.Module):\n",
    "    \"\"\"A classifier that *reuses* a frozen embedder’s conv+LSTM and learns a\n",
    "    fresh classification head. Optionally fine‑tunes the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: DeepConvLSTMEmbedder, n_classes: int,\n",
    "                 train_encoder: bool = False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        # freeze?\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = train_encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(encoder.embedding_dim, encoder.embedding_dim//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(encoder.embedding_dim//2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(encoder.embedding_dim//2, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        with torch.set_grad_enabled(self.encoder.feature_extractor[0].weight.requires_grad):\n",
    "            emb, _ = self.encoder(x)\n",
    "        return self.head(emb)\n",
    "\n",
    "\n",
    "class WearDataset(Dataset):\n",
    "    \"\"\"Takes a list of (x,y,z arrays, label) tuples.\"\"\"\n",
    "    def __init__(self, samples: list[tuple[torch.Tensor, int]]):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label = self.samples[idx]\n",
    "        return signal.float(), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def build_dataloaders(data_dir: Path, loc: str, batch: int = 128) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Reads `<loc>_windows.csv`, parses signals, splits by subject into train/test sets.\"\"\"\n",
    "    import random\n",
    "    csv_path = data_dir / f\"{loc}_windows.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    subj_ids = sorted(df['sbj_id'].unique().tolist())\n",
    "    random.seed(42)\n",
    "    test_subjs = random.sample(subj_ids, k=2)\n",
    "    train_df = df[~df['sbj_id'].isin(test_subjs)]\n",
    "    test_df  = df[df['sbj_id'].isin(test_subjs)]\n",
    "\n",
    "    def df_to_samples(dataframe):\n",
    "        samples: list[tuple[torch.Tensor, int]] = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            x = torch.tensor(ast.literal_eval(row['x_axis']), dtype=torch.float)\n",
    "            y = torch.tensor(ast.literal_eval(row['y_axis']), dtype=torch.float)\n",
    "            z = torch.tensor(ast.literal_eval(row['z_axis']), dtype=torch.float)\n",
    "            sig = torch.stack([x, y, z], dim=0)\n",
    "            lbl = int(row['label'])\n",
    "            samples.append((sig, lbl))\n",
    "        return samples\n",
    "\n",
    "    train_samples = df_to_samples(train_df)\n",
    "    test_samples  = df_to_samples(test_df)\n",
    "\n",
    "    train_labels = [lbl for _, lbl in train_samples]\n",
    "    class_counts = pd.Series(train_labels).value_counts().sort_index().values\n",
    "    weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "    samples_weight = torch.tensor([weights[lbl] for _, lbl in train_samples])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    train_loader = DataLoader(WearDataset(train_samples), batch_size=batch,\n",
    "                              sampler=sampler, drop_last=True)\n",
    "    test_loader  = DataLoader(WearDataset(test_samples),  batch_size=batch,\n",
    "                              shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Training utilities\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total      += y.size(0)\n",
    "        correct    += (logits.argmax(1) == y).sum().item()\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total      += y.size(0)\n",
    "        correct    += (logits.argmax(1) == y).sum().item()\n",
    "    return total_loss/total, correct/total\n"
   ],
   "id": "a05f829bda812c03",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T08:24:06.210548Z",
     "start_time": "2025-06-25T08:24:06.204285Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e1c21d4cb113309e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loader, val_loader = build_dataloaders(TRAIN_DIR,LOCATION_IDS[0], 128)\n",
    "device = torch.device(DEVICE)\n",
    "if TRUE:\n",
    "    model = DeepConvLSTMEmbedder(n_classes=len(list(label_map.keys()))).to(device)\n",
    "else:  # finetune\n",
    "    enc = DeepConvLSTMEmbedder(n_classes=len(list(label_map.keys())))\n",
    "    enc.load_state_dict(torch.load(f\"embedder_{args.loc}.pt\"))\n",
    "    model = DeepConvLSTMClassifier(enc, args.classes, train_encoder=False).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    va_loss, va_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d}: train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "    # basic checkpointing\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        filename = (\n",
    "            f\"{'embedder' if args.mode=='pretrain' else 'clf'}_{args.loc}.pt\"\n",
    "        )\n",
    "        torch.save(model.state_dict(), filename)"
   ],
   "id": "2581055a484fb9fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "719bd9cba9117e38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 83,
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score\n",
    "from ast import literal_eval\n",
    "from numpy import nan\n",
    "\n",
    "class WearModelBuilder:\n",
    "    def __init__(self, num_classes=19):\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {\n",
    "            'null': 0, 'jogging': 1, 'jogging (rotating arms)': 2,\n",
    "            'jogging (skipping)': 3, 'jogging (butt-kicks)': 4,\n",
    "            'stretching (triceps)': 5, 'stretching (lunging)': 6,\n",
    "            'stretching (shoulders)': 7, 'stretching (hamstrings)': 8,\n",
    "            'stretching (lumbar rotation)': 9, 'push-ups': 10,\n",
    "            'push-ups (complex)': 11, 'sit-ups': 12,\n",
    "            'sit-ups (complex)': 13, 'burpees': 14,\n",
    "            'lunges': 15, 'lunges (complex)': 16,\n",
    "            'bench-dips': 17\n",
    "        }\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            df = pd.read_csv(f)\n",
    "        df['x_axis'] = df['x_axis'].apply(lambda row: eval(row))\n",
    "        df['y_axis'] = df['y_axis'].apply(lambda row: eval(row))\n",
    "        df['z_axis'] = df['z_axis'].apply(lambda row: eval(row))\n",
    "        df['sbj_id'] = df['sbj_id'].astype(int)\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        return df\n",
    "\n",
    "    def prepare_datasets(self, df, test_size=0.2, random_state=42):\n",
    "        subjects = df['sbj_id'].unique()\n",
    "        train_subjects, test_subjects = train_test_split(\n",
    "            subjects, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        train_df = df[df['sbj_id'].isin(train_subjects)]\n",
    "        test_df = df[df['sbj_id'].isin(test_subjects)]\n",
    "\n",
    "        return train_df, test_df\n",
    "\n",
    "    def build_conv_lstm_model(self, input_shape):\n",
    "        \"\"\"Build a ConvLSTM model based on challenge recommendations\"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Conv block 1\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Conv block 2\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(128, return_sequences=False)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # Output\n",
    "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_model(self, train_df, test_df, epochs=50, batch_size=32):\n",
    "        \"\"\"Train a model on the given data\"\"\"\n",
    "        train_df[\"x_axis\"]=train_df[\"x_axis\"].apply(lambda row: np.array(row))\n",
    "        train_df[\"y_axis\"]=train_df[\"y_axis\"].apply(lambda row: np.array(row))\n",
    "        train_df[\"z_axis\"]=train_df[\"z_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"x_axis\"]=test_df[\"x_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"y_axis\"]=test_df[\"y_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"z_axis\"]=test_df[\"z_axis\"].apply(lambda row: np.array(row))\n",
    "        X_train = train_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "        y_train = train_df['label'].values\n",
    "        X_test = test_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "        y_test = test_df['label'].values\n",
    "        train_array = np.zeros((X_train.shape[0], 3, 50))\n",
    "        for i in range(X_train.shape[0]):\n",
    "            for j in range(X_train.shape[1]):\n",
    "                train_array[i][j]=np.array(X_train[i][j])\n",
    "        test_array = np.zeros((X_test.shape[0], 3, 50))\n",
    "        for i in range(X_test.shape[0]):\n",
    "            for j in range(X_test.shape[1]):\n",
    "                test_array[i][j]=np.array(X_test[i][j])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(np.array([x.flatten() for x in train_array])).reshape(train_array.shape)\n",
    "        X_test = scaler.transform(np.array([x.flatten() for x in test_array])).reshape(test_array.shape)\n",
    "\n",
    "        y_train = to_categorical(y_train, num_classes=self.num_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=self.num_classes)\n",
    "\n",
    "        model = self.build_conv_lstm_model(input_shape=(3, 50))\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_true = test_df['label'].values\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "        return model, history, f1"
   ],
   "id": "98a96c44aabfa38b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b40893f30ad20e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for entry in LOCATION_IDS[1:]:\n",
    "    wear_model_builder = WearModelBuilder()\n",
    "    df = wear_model_builder.load_data(TRAIN_DIR/f\"{entry}_windows.csv\")\n",
    "    result = wearModelBuilder.prepare_datasets(df)\n",
    "    wear_model_builder.build_conv_lstm_model(result[0].shape)\n",
    "    model = wear_model_builder.train_model(result[0], result[1])\n",
    "    model[0].save(WORK_DIR/f\"model-{entry}.h5\")"
   ],
   "id": "72cd8cc6bbaeb4c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T22:05:56.934369Z",
     "start_time": "2025-06-24T22:05:55.737408Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "def88cdff7e6dcb7",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T22:24:51.717285Z",
     "start_time": "2025-06-24T22:24:32.571752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "for entry in LOCATION_IDS:\n",
    "test_df[\"x_axis\"]=test_df[\"x_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "test_df[\"y_axis\"]=test_df[\"y_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "test_df[\"z_axis\"]=test_df[\"z_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "X_test = test_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "test_array = np.zeros((X_test.shape[0], 3, 50))\n",
    "for i in range(X_test.shape[0]):\n",
    "    for j in range(X_test.shape[1]):\n",
    "        test_array[i][j]=np.array(X_test[i][j])\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(np.array([x.flatten() for x in test_array])).reshape(test_array.shape)\n",
    "    model = tf.keras.models.load_model(WORK_DIR/f\"model-{entry}.h5\")\n",
    "\n",
    "\n",
    "results = model[0].predict(X_test)\n",
    "result_df = pd.DataFrame([list(test_df[\"id\"].values), list([np.argmax(x) for x in results])]).transpose().rename(columns={0: 'id', 1: 'label'})\n",
    "result_df.to_csv(\"submission.csv\", index=False)"
   ],
   "id": "114e5b8349e6f738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1530/1530\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 1ms/step\n"
     ]
    }
   ],
   "execution_count": 81
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
