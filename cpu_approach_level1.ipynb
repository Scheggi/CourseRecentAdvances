{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T23:58:31.365249Z",
     "start_time": "2025-06-22T23:58:28.954107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, math, random, json, warnings, itertools, time, gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "RAW_DIR   = Path('data')\n",
    "TRAIN_DIR = RAW_DIR / 'train'\n",
    "TRAIN_CSV = sorted(TRAIN_DIR.glob('sbj_*.csv'))\n",
    "TEST_CSV  = RAW_DIR / 'test.csv'\n",
    "LOCATION_IDS = [\"right_arm\", \"left_arm\", \"right_leg\", \"left_leg\"]\n",
    "WORK_DIR = Path('work1')\n",
    "WIN_SIZE  = 50\n",
    "STRIDE    = 25\n",
    "BATCH_SZ  = 256\n",
    "EPOCHS    = 20\n",
    "EMB_DIM   = 64\n",
    "LR        = 1e-3\n",
    "NUM_WORKERS = 4\n",
    "DEVICE    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "loc = \"right_arm\""
   ],
   "id": "c0e8c71361793e48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "db8779d3",
   "metadata": {},
   "source": [
    "### 1. Generate per‑location CSVs (adds `subject_id`)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2c181ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T22:33:41.299880Z",
     "start_time": "2025-06-22T22:32:45.360356Z"
    }
   },
   "source": [
    "def make_location_csvs():\n",
    "    WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for loc in LOCATION_IDS:\n",
    "        cols = [f\"{loc}_acc_{ax}\" for ax in ['x','y','z']] + ['label']\n",
    "        frames = []\n",
    "        for csv_file in TRAIN_CSV:\n",
    "            df = pd.read_csv(csv_file, usecols=lambda c: c in cols)\n",
    "            subj_id = int(Path(csv_file).stem.split('_')[-1])\n",
    "            df['subject_id'] = subj_id\n",
    "            df = df.dropna(subset=[f\"{loc}_acc_x\", f\"{loc}_acc_y\", f\"{loc}_acc_z\", 'label'])\n",
    "            frames.append(df)\n",
    "        combined = pd.concat(frames, ignore_index=True)\n",
    "        out_path = WORK_DIR / f\"train_{loc}.csv\"\n",
    "        combined.to_csv(out_path, index=False)\n",
    "        print(f\"Written {out_path}: {len(combined)} rows\")\n",
    "\n",
    "\n",
    "make_location_csvs()\n",
    "print(\"Generated 4 train CSVs in WORK_DIR.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written work1/train_right_arm.csv: 2089038 rows\n",
      "Written work1/train_left_arm.csv: 2054764 rows\n",
      "Written work1/train_right_leg.csv: 2089038 rows\n",
      "Written work1/train_left_leg.csv: 2089038 rows\n",
      "Generated 4 train CSVs in WORK_DIR.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "8f75d50e",
   "metadata": {},
   "source": [
    "### 2. Dataset & augmentations"
   ]
  },
  {
   "cell_type": "code",
   "id": "45a1aaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T23:58:34.753807Z",
     "start_time": "2025-06-22T23:58:34.746974Z"
    }
   },
   "source": [
    "\n",
    "def jitter(x, sigma=0.015):\n",
    "    return x + torch.randn_like(x) * sigma\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    factor = torch.normal(1.0, sigma, (x.size(0), 1), device=x.device)\n",
    "    return x * factor\n",
    "\n",
    "def rotation(x):\n",
    "    B, C, T = x.shape\n",
    "    angles = torch.randn(B, 3, device=x.device) * 0.2\n",
    "    Rx = torch.tensor([[1,0,0],[0, torch.cos(angles[:,0]), -torch.sin(angles[:,0])],\n",
    "                       [0, torch.sin(angles[:,0]),  torch.cos(angles[:,0])]])\n",
    "    return x\n",
    "\n",
    "class WearWindowDataset(Dataset):\n",
    "    def __init__(self, df, scaler=None, train=True, augment=True):\n",
    "        self.train = train\n",
    "        self.augment = augment and train\n",
    "\n",
    "        loc = df.columns[0].split('_acc_')[0]\n",
    "\n",
    "        feats = df[[f\"{loc}_acc_x\", f\"{loc}_acc_y\", f\"{loc}_acc_z\"]].values.astype('float32')\n",
    "\n",
    "        labels = df['label'].values.astype('int64') if 'label' in df.columns else None\n",
    "        X, y = [], []\n",
    "        for i in range(0, len(feats) - WIN_SIZE + 1, STRIDE):\n",
    "            window = feats[i : i + WIN_SIZE]                # shape (50, 3)\n",
    "            if np.isnan(window).any():\n",
    "                continue\n",
    "            X.append(window)                                # still (50, 3)\n",
    "            if labels is not None:\n",
    "                y.append(np.bincount(labels[i : i + WIN_SIZE]).argmax())\n",
    "\n",
    "        self.X = np.stack(X)                                # (N_windows, 50, 3)\n",
    "        self.y = np.array(y) if labels is not None else None\n",
    "\n",
    "        if scaler is not None:\n",
    "            flat = self.X.reshape(-1, 3)\n",
    "            flat = scaler.transform(flat)\n",
    "            self.X = flat.reshape(-1, WIN_SIZE, 3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # from (50,3) to (3,50) for Conv1d\n",
    "        x = torch.tensor(self.X[idx]).permute(1, 0)   # now (3,50)\n",
    "        if self.augment:\n",
    "            x = jitter(x)\n",
    "            x = scaling(x)\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, torch.tensor(self.y[idx])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "783d9e8f",
   "metadata": {},
   "source": [
    "### 3. Shallow DeepConvLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "id": "32867a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T23:58:35.885902Z",
     "start_time": "2025-06-22T23:58:35.882086Z"
    }
   },
   "source": [
    "\n",
    "class ShallowDeepConvLSTM(nn.Module):\n",
    "    def __init__(self, n_classes, emb_dim=EMB_DIM):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.embed = nn.Sequential(nn.Dropout(0.3), nn.Linear(128, emb_dim), nn.ReLU(inplace=True))\n",
    "        self.head  = nn.Linear(emb_dim, n_classes)\n",
    "    def forward(self, x):          # x (B, C=3, T=50)\n",
    "        x = self.conv(x)           # (B, 64, T)\n",
    "        x = x.permute(0,2,1)       # (B, T, 64)\n",
    "        _, (h, _) = self.lstm(x)   # h (1, B, 128)\n",
    "        emb = self.embed(h[-1])    # (B, emb_dim)\n",
    "        return self.head(emb)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "9822320a",
   "metadata": {},
   "source": [
    "### 4. Focal loss & training utilities"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef740f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T23:58:37.379826Z",
     "start_time": "2025-06-22T23:58:37.375347Z"
    }
   },
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none', weight=alpha)\n",
    "    def forward(self, logits, target):\n",
    "        ce_loss = self.ce(logits, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal = ((1-pt) ** self.gamma) * ce_loss\n",
    "        return focal.mean()\n",
    "\n",
    "def run_epoch(model, loader, optim=None, scheduler=None, criterion=None):\n",
    "    train = optim is not None\n",
    "    model.train(train)\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            if train:\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if scheduler and train:\n",
    "        scheduler.step()\n",
    "    return float(np.mean(losses))\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "4cabc249",
   "metadata": {},
   "source": [
    "### 5. Training ‑‑ Leave‑One‑Subject‑Out CV"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a761b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T23:58:39.879576Z",
     "start_time": "2025-06-22T23:58:39.871897Z"
    }
   },
   "source": [
    "\n",
    "def train_location(location_csv, location):\n",
    "    df = pd.read_csv(location_csv)\n",
    "    df['label'] = df['label'].map(label_map).astype(int)\n",
    "    subjects = df['subject_id'].unique()\n",
    "    fold_f1 = []\n",
    "\n",
    "    for val_subj in subjects:\n",
    "        train_df = df[df.subject_id != val_subj].reset_index(drop=True)\n",
    "        val_df   = df[df.subject_id == val_subj].reset_index(drop=True)\n",
    "        scaler   = StandardScaler().fit(train_df.filter(like=f\"{location}_acc_\"))\n",
    "        train_ds = WearWindowDataset(train_df, scaler, train=True, augment=True)\n",
    "        val_ds   = WearWindowDataset(val_df,   scaler, train=False, augment=False)\n",
    "        counts = np.bincount(train_ds.y, minlength=len(label_map))\n",
    "        sampler =  torch.utils.data.WeightedRandomSampler(\n",
    "            weights=1.0 / np.maximum(counts, 1),\n",
    "            num_samples=len(train_ds.y),\n",
    "            replacement=True\n",
    "        )\n",
    "        train_dl = DataLoader(train_ds, batch_size=BATCH_SZ, sampler=sampler, shuffle=False,num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        val_dl   = DataLoader(val_ds,   batch_size=BATCH_SZ, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "        n_classes = len(label_map)\n",
    "        model     = ShallowDeepConvLSTM(n_classes).to(DEVICE)\n",
    "        optim     = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "        sched     = CosineAnnealingLR(optim, T_max=EPOCHS)\n",
    "        loss_fn   = FocalLoss(alpha=None)\n",
    "\n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            loss = run_epoch(model, train_dl, optim, sched, loss_fn)\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                x = x.to(DEVICE)\n",
    "                logits = model(x)\n",
    "                y_true.append(y.numpy())\n",
    "                y_pred.append(logits.argmax(1).cpu().numpy())\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        fold_f1.append(f1)\n",
    "\n",
    "        ckpt = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"scaler_mean\": scaler.mean_,\n",
    "            \"scaler_scale\": scaler.scale_\n",
    "        }\n",
    "        ckpt_path = WORK_DIR / f\"model_{location}_fold{val_subj}.pt\"\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        print(f\"→ {location} | held-out subj {val_subj} | F1={f1:.4f}\")\n",
    "\n",
    "    mean_f1 = float(np.mean(fold_f1))\n",
    "    print(f\"=== {location} LOSO mean F1: {mean_f1:.4f} ===\")\n",
    "    return mean_f1\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1cff0099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T03:02:28.533864Z",
     "start_time": "2025-06-22T23:58:44.016772Z"
    }
   },
   "source": [
    "\n",
    "location_scores = {}\n",
    "#for loc in LOCATION_IDS:\n",
    "score = train_location(WORK_DIR/f'train_{loc}.csv','right_arm')\n",
    "location_scores[loc] = score\n",
    "print('LOSO scores per location:', location_scores)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ right_arm | held-out subj 0 | F1=0.0103\n",
      "→ right_arm | held-out subj 2 | F1=0.0072\n",
      "→ right_arm | held-out subj 1 | F1=0.0219\n",
      "→ right_arm | held-out subj 10 | F1=0.0105\n",
      "→ right_arm | held-out subj 11 | F1=0.0231\n",
      "→ right_arm | held-out subj 12 | F1=0.0219\n",
      "→ right_arm | held-out subj 13 | F1=0.0164\n",
      "→ right_arm | held-out subj 14 | F1=0.0099\n",
      "→ right_arm | held-out subj 15 | F1=0.0091\n",
      "→ right_arm | held-out subj 16 | F1=0.0138\n",
      "→ right_arm | held-out subj 17 | F1=0.0243\n",
      "→ right_arm | held-out subj 18 | F1=0.0120\n",
      "→ right_arm | held-out subj 19 | F1=0.0244\n",
      "→ right_arm | held-out subj 20 | F1=0.0109\n",
      "→ right_arm | held-out subj 21 | F1=0.0112\n",
      "→ right_arm | held-out subj 3 | F1=0.0172\n",
      "→ right_arm | held-out subj 4 | F1=0.0178\n",
      "→ right_arm | held-out subj 5 | F1=0.0189\n",
      "→ right_arm | held-out subj 6 | F1=0.0117\n",
      "→ right_arm | held-out subj 7 | F1=0.0156\n",
      "→ right_arm | held-out subj 8 | F1=0.0301\n",
      "→ right_arm | held-out subj 9 | F1=0.0139\n",
      "=== right_arm LOSO mean F1: 0.0160 ===\n",
      "LOSO scores per location: {'right_arm': 0.015998925475530775}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ab8c0390",
   "metadata": {},
   "source": "t### 6. Train on full data & predict test set (creates `submission.csv`)"
  },
  {
   "cell_type": "code",
   "id": "7cd99d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:13:39.392990Z",
     "start_time": "2025-06-23T12:13:39.227937Z"
    }
   },
   "source": [
    "\n",
    "def predict_test(model, df, scaler):\n",
    "    ds = WearWindowDataset(df, scaler, train=False, augment=False)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SZ, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    model.eval(); preds = []\n",
    "    with torch.no_grad():\n",
    "        for x in dl:\n",
    "            x = x.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            preds.append(logits.argmax(1).cpu().numpy())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "subs = []\n",
    "for loc in ['right_arm']:# LOCATION_IDS:\n",
    "    train_df = pd.read_csv(f'train_{loc}.csv')\n",
    "    test_df  = pd.read_csv(f'test_{loc}.csv')\n",
    "    scaler   = StandardScaler().fit(train_df.filter(regex='_acc_'))\n",
    "    train_ds = WearWindowDataset(train_df, scaler, train=True, augment=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SZ, shuffle=True)\n",
    "    n_classes = train_df['label'].max() + 1\n",
    "    class_weights = compute_class_weight('balanced', classes=np.arange(n_classes), y=train_ds.y)\n",
    "    model = ShallowDeepConvLSTM(n_classes).to(DEVICE)\n",
    "    optim = AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = CosineAnnealingLR(optim, T_max=EPOCHS)\n",
    "    crit = FocalLoss(alpha=torch.tensor(class_weights, device=DEVICE))\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        run_epoch(model, train_loader, optim, scheduler, crit)\n",
    "    preds = predict_test(model, test_df, scaler)\n",
    "    sub_df = pd.DataFrame({'row_id': test_df['row_id'], 'label': preds})\n",
    "    subs.append(sub_df)\n",
    "\n",
    "submission = pd.concat(subs).sort_values('row_id')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv')\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_right_arm.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m subs = []\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m loc \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mright_arm\u001B[39m\u001B[33m'\u001B[39m]:\u001B[38;5;66;03m# LOCATION_IDS:\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     train_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtrain_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mloc\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m     test_df  = pd.read_csv(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtest_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     16\u001B[39m     scaler   = StandardScaler().fit(train_df.filter(regex=\u001B[33m'\u001B[39m\u001B[33m_acc_\u001B[39m\u001B[33m'\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/hasca-wear/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'train_right_arm.csv'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:18:41.700808Z",
     "start_time": "2025-06-23T12:18:41.694684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Union, Sequence\n",
    "\n",
    "def merge_fold_checkpoints(\n",
    "    fold_dir: Union[str, Path],\n",
    "    pattern: str = \"model_right_arm_fold*.pt\",\n",
    "    output_path: Union[str, Path] = \"model_right_arm_averaged.pt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Averages the weights of all fold checkpoints matching <pattern> in <fold_dir>\n",
    "    and writes a single averaged checkpoint to <output_path>.\n",
    "\n",
    "    fold_dir:     directory containing per-fold .pt files\n",
    "    pattern:      glob pattern to match your fold files\n",
    "    output_path:  path for the merged checkpoint\n",
    "    \"\"\"\n",
    "    fold_dir   = Path(fold_dir)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    torch.serialization.add_safe_globals([\"numpy.core.multiarray._reconstruct\"])\n",
    "\n",
    "    fold_files = sorted(fold_dir.glob(pattern))\n",
    "    if not fold_files:\n",
    "        raise FileNotFoundError(f\"No files matching {pattern} in {fold_dir}\")\n",
    "\n",
    "    state_dicts = []\n",
    "    for p in fold_files:\n",
    "        ckpt = torch.load(p, map_location=\"cpu\", weights_only=False)\n",
    "        sd   = ckpt.get(\"state_dict\", ckpt)\n",
    "        state_dicts.append(sd)\n",
    "\n",
    "    avg_sd = {}\n",
    "    keys = state_dicts[0].keys()\n",
    "    for key in keys:\n",
    "        # stack along new dim=0 then mean\n",
    "        stacked = torch.stack([sd[key].float() for sd in state_dicts], dim=0)\n",
    "        avg_sd[key] = stacked.mean(dim=0)\n",
    "\n",
    "    torch.save({\"state_dict\": avg_sd}, output_path)\n",
    "    print(f\"✅ Merged {len(fold_files)} checkpoints → {output_path}\")\n"
   ],
   "id": "3a6cee30a7c6676e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:18:46.760300Z",
     "start_time": "2025-06-23T12:18:46.725823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merge_fold_checkpoints(\n",
    "    fold_dir=\"work1\",\n",
    "    pattern=\"model_right_arm_fold*.pt\",\n",
    "    output_path=\"work1/model_right_arm_averaged.pt\"\n",
    ")\n"
   ],
   "id": "f7855e7f083119a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged 22 checkpoints → work1/model_right_arm_averaged.pt\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:19:07.378116Z",
     "start_time": "2025-06-23T12:19:07.365313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt = torch.load(\"work1/model_right_arm_averaged.pt\", map_location=DEVICE)\n",
    "model = ShallowDeepConvLSTM(n_classes=len(label_map)).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n"
   ],
   "id": "8cf8ba4b9df3d974",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShallowDeepConvLSTM(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv1d(3, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lstm): LSTM(64, 128, batch_first=True)\n",
       "  (embed): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (head): Linear(in_features=64, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:02:53.245891Z",
     "start_time": "2025-06-23T13:02:53.240024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WearWindowDataset(Dataset):\n",
    "    def __init__(self, df, scaler=None, train=True, augment=True):\n",
    "        self.train   = train\n",
    "        self.augment = augment and train\n",
    "\n",
    "        acc_x_cols = [c for c in df.columns if c.endswith('_acc_x')]\n",
    "        if acc_x_cols:\n",
    "            loc = acc_x_cols[0].split('_acc_')[0]\n",
    "            axes = [f\"{loc}_acc_{ax}\" for ax in ('x','y','z')]\n",
    "        else:\n",
    "            axes = [c for c in df.columns if c.endswith('_axis')]\n",
    "            if sorted(axes) != ['x_axis', 'y_axis', 'z_axis']:\n",
    "                raise ValueError(f\"Unrecognized axis columns: {axes}\")\n",
    "\n",
    "        feats = df[axes].values.astype('float32')   # (N_rows, 3)\n",
    "        labels = df['label'].values.astype('int64') if 'label' in df.columns else None\n",
    "\n",
    "        X, y = [], []\n",
    "        for i in range(0, len(feats)-WIN_SIZE+1, STRIDE):\n",
    "            w = feats[i:i+WIN_SIZE]\n",
    "            if np.isnan(w).any():\n",
    "                continue\n",
    "            X.append(w)\n",
    "            if labels is not None:\n",
    "                y.append(np.bincount(labels[i:i+WIN_SIZE]).argmax())\n",
    "\n",
    "        self.X = np.stack(X)                        # (N_windows, 50, 3)\n",
    "        self.y = np.array(y) if labels is not None else None\n",
    "\n",
    "        if scaler is not None:\n",
    "            flat = self.X.reshape(-1, 3)\n",
    "            flat = scaler.transform(flat)\n",
    "            self.X = flat.reshape(-1, WIN_SIZE, 3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # (50,3) -> (3,50)\n",
    "        x = torch.tensor(self.X[idx]).permute(1, 0)\n",
    "        if self.augment:\n",
    "            x = jitter(x)\n",
    "            x = scaling(x)\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, torch.tensor(self.y[idx])\n"
   ],
   "id": "fa4bd3feb4f5bf45",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:11:31.563113Z",
     "start_time": "2025-06-23T13:10:13.216604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "ckpt  = torch.load(\"work1/model_right_arm_averaged.pt\", map_location=DEVICE)\n",
    "model = ShallowDeepConvLSTM(n_classes=len(label_map)).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "df_train = pd.read_csv(WORK_DIR/\"train_right_arm.csv\")\n",
    "train_axes = [c for c in df_train.columns if c.endswith(\"_acc_x\")]\n",
    "loc        = train_axes[0].split(\"_acc_\")[0]\n",
    "scaler     = StandardScaler().fit(\n",
    "    df_train[[f\"{loc}_acc_{ax}\" for ax in (\"x\",\"y\",\"z\")]]\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "results = []\n",
    "\n",
    "WIN = 50\n",
    "STRIDE = 25\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    rid    = row[\"id\"]\n",
    "    x_full = np.array(ast.literal_eval(row[\"x_axis\"]), dtype=np.float32)\n",
    "    y_full = np.array(ast.literal_eval(row[\"y_axis\"]), dtype=np.float32)\n",
    "    z_full = np.array(ast.literal_eval(row[\"z_axis\"]), dtype=np.float32)\n",
    "\n",
    "    L = len(x_full)\n",
    "    window_preds = []\n",
    "\n",
    "    for i in range(0, L - WIN + 1, STRIDE):\n",
    "        w = np.stack([x_full[i:i+WIN],\n",
    "                      y_full[i:i+WIN],\n",
    "                      z_full[i:i+WIN]], axis=1)  # (50,3)\n",
    "        flat = scaler.transform(w)            # (50,3)\n",
    "        win  = torch.tensor(flat.T).unsqueeze(0).to(DEVICE)  # (1,3,50)\n",
    "        with torch.no_grad():\n",
    "            logits = model(win)               # (1, n_classes)\n",
    "        window_preds.append(int(logits.argmax(1).cpu().item()))\n",
    "\n",
    "    if not window_preds:\n",
    "        w = np.stack([x_full, y_full, z_full], axis=1)\n",
    "        flat = scaler.transform(\n",
    "            np.pad(w, ((0, max(0, WIN-L)), (0,0)), mode='edge')\n",
    "        )\n",
    "        win  = torch.tensor(flat.T).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            window_preds.append(int(model(win).argmax(1).cpu().item()))\n",
    "\n",
    "    label = Counter(window_preds).most_common(1)[0][0]\n",
    "    results.append((rid, label))\n",
    "\n",
    "out_df = pd.DataFrame(results, columns=[\"id\",\"label\"])\n",
    "out_df.to_csv(\"result.csv\", index=False)\n",
    "print(\"✅ Wrote result.csv with\", len(out_df), \"rows.\")\n"
   ],
   "id": "d73f96eb6a268822",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote result.csv with 48936 rows.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "714bdc066d44107b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
