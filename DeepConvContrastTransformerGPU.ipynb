{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:24.185746Z",
     "start_time": "2025-07-09T18:02:16.066603Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:24.386042Z",
     "start_time": "2025-07-09T18:02:24.377092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_label(lbl):\n",
    "    mapping = {\n",
    "        'null': 0,\n",
    "        'jogging': 1,\n",
    "        'jogging (rotating arms)': 2,\n",
    "        'jogging (skipping)': 3,\n",
    "        'jogging (sidesteps)': 4,\n",
    "        'jogging (butt-kicks)': 5,\n",
    "        'stretching (triceps)': 6,\n",
    "        'stretching (lunging)': 7,\n",
    "        'stretching (shoulders)': 8,\n",
    "        'stretching (hamstrings)': 9,\n",
    "        'stretching (lumbar rotation)': 10,\n",
    "        'push-ups': 11,\n",
    "        'push-ups (complex)': 12,\n",
    "        'sit-ups': 13,\n",
    "        'sit-ups (complex)': 14,\n",
    "        'burpees': 15,\n",
    "        'lunges': 16,\n",
    "        'lunges (complex)': 17,\n",
    "        'bench-dips': 18\n",
    "    }\n",
    "    return mapping.get(lbl, np.nan)\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}"
   ],
   "id": "489c10a75fd263e9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:24.499231Z",
     "start_time": "2025-07-09T18:02:24.490319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "a8c72f945b28c33e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:24.568862Z",
     "start_time": "2025-07-09T18:02:24.564820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = Path('data')\n",
    "train_dir = data_dir / 'train'\n",
    "meta_file = data_dir / 'meta_data.txt'\n",
    "test_file = data_dir/'test.csv'"
   ],
   "id": "14620b65809e956c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:41.734638Z",
     "start_time": "2025-07-09T18:02:24.619566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sbj_files = sorted(train_dir.glob('sbj_*.csv'))\n",
    "dfs = []\n",
    "for f in sbj_files:\n",
    "    df = pd.read_csv(f,low_memory=False)\n",
    "    df['subject'] = df['sbj_id'].astype(str)\n",
    "    dfs.append(df)\n",
    "\n",
    "raw_df = pd.concat(dfs, ignore_index=True)\n"
   ],
   "id": "e55c347192a068fd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:56.920035Z",
     "start_time": "2025-07-09T18:02:41.767394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_df['label_code'] = raw_df['label'].apply(map_label)\n",
    "raw_df = raw_df.dropna(subset=['label_code']).reset_index(drop=True)\n",
    "raw_df['label_code'] = raw_df['label_code'].astype(int)\n",
    "\n",
    "sensor_cols = [c for c in raw_df.columns if c not in ['sbj_id', 'subject', 'label', 'label_code']]\n",
    "raw_df = raw_df.dropna(subset=sensor_cols).reset_index(drop=True)\n",
    "scaler = StandardScaler()\n",
    "raw_df[sensor_cols] = scaler.fit_transform(raw_df[sensor_cols])"
   ],
   "id": "f9054cd986665ed0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:56.979896Z",
     "start_time": "2025-07-09T18:02:56.975421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequences(df, sensor_cols, target_col, window, step):\n",
    "    X, y = [], []\n",
    "    data = df[sensor_cols].values\n",
    "    labels = df[target_col].values\n",
    "    for start in range(0, len(df) - window + 1, step):\n",
    "        end = start + window\n",
    "        seq = data[start:end]\n",
    "        lab = np.bincount(labels[start:end]).argmax()\n",
    "        X.append(seq)\n",
    "        y.append(lab)\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "4b9c676a4aa0d699",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:57.055038Z",
     "start_time": "2025-07-09T18:02:57.050542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]"
   ],
   "id": "881e125c2e6930b5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:57.134392Z",
     "start_time": "2025-07-09T18:02:57.127542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def jitter(x, sigma=0.8):\n",
    "    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n",
    "\n",
    "\n",
    "def scaling(x, sigma=1.1):\n",
    "    \"\"\"\n",
    "    Applies scaling to a 2D tensor of shape (sequence_length, features).\n",
    "    \"\"\"\n",
    "    # The factor should have the same shape as the input to allow for element-wise multiplication.\n",
    "    factor = np.random.normal(loc=1., scale=sigma, size=x.shape)\n",
    "    return x * factor\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        # Apply two different augmentations\n",
    "        x1 = torch.tensor(jitter(x.numpy()), dtype=torch.float32)\n",
    "        x2 = torch.tensor(scaling(x.numpy()), dtype=torch.float32)\n",
    "\n",
    "        return x1, x2, y"
   ],
   "id": "cb657f0ca05f1549",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:57.211228Z",
     "start_time": "2025-07-09T18:02:57.201077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature=0.1, use_cosine_similarity=True):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits /= self.temperature\n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss / (2 * self.batch_size)"
   ],
   "id": "4a2cb40961bd01b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:57.289724Z",
     "start_time": "2025-07-09T18:02:57.283600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepConvLSTM_contrastive(nn.Module):\n",
    "    def __init__(self, num_channels, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # Backbone\n",
    "        self.conv1 = nn.Conv1d(num_channels, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(128, 128, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Projection Head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone forward pass\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x)); x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x)); x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x)); x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        features = out[:, -1, :]\n",
    "\n",
    "        # Projection head forward pass\n",
    "        projection = self.projection(features)\n",
    "\n",
    "        return features, projection"
   ],
   "id": "45d190927f7c902f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:02:57.361262Z",
     "start_time": "2025-07-09T18:02:57.352059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_tokens=16, num_heads=4, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure the input can be reshaped into tokens cleanly\n",
    "        assert input_dim % num_tokens == 0, \"input_dim must be divisible by num_tokens\"\n",
    "        self.token_dim = input_dim // num_tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # A standard PyTorch Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.token_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important: ensures input format is (batch, seq, feature)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # A linear layer to make the final classification\n",
    "        self.classifier_head = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x starts with shape: (batch_size, input_dim), e.g., (64, 128)\n",
    "\n",
    "        # 1. Reshape the vector into a sequence of tokens\n",
    "        # Shape becomes: (batch_size, num_tokens, token_dim), e.g., (64, 16, 8)\n",
    "        x = x.view(-1, self.num_tokens, self.token_dim)\n",
    "\n",
    "        # 2. Pass the sequence through the Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "\n",
    "        # 3. Flatten the output sequence back into a single vector\n",
    "        # Shape becomes: (batch_size, input_dim), e.g., (64, 128)\n",
    "        x_flat = transformer_output.reshape(-1, self.input_dim)\n",
    "\n",
    "        # 4. Make the final prediction\n",
    "        return self.classifier_head(x_flat)\n"
   ],
   "id": "aedcb4612051042e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:03:02.192018Z",
     "start_time": "2025-07-09T18:02:57.434231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "raw_df_filtered = raw_df[raw_df['label_code'] != 0].copy()\n",
    "\n",
    "# Create a new mapping for the remaining labels (1-18 -> 0-17)\n",
    "# This is crucial for the model's loss function\n",
    "original_labels = sorted(raw_df_filtered['label_code'].unique())\n",
    "label_remapping = {orig_label: new_label for new_label, orig_label in enumerate(original_labels)}\n",
    "raw_df_filtered['remapped_label'] = raw_df_filtered['label_code'].map(label_remapping)\n",
    "right_arm_df = raw_df_filtered[[\"right_arm_acc_x\", \"right_arm_acc_y\", \"right_arm_acc_z\", \"subject\", \"remapped_label\"]]\n",
    "left_arm_df = raw_df_filtered[[\"left_arm_acc_x\", \"left_arm_acc_y\", \"left_arm_acc_z\", \"subject\", \"remapped_label\"]]\n",
    "right_leg_df = raw_df_filtered[[\"right_leg_acc_x\", \"right_leg_acc_y\", \"right_leg_acc_z\", \"subject\", \"remapped_label\"]]\n",
    "left_leg_df = raw_df_filtered[[\"left_leg_acc_x\", \"left_leg_acc_y\", \"left_leg_acc_z\", \"subject\", \"remapped_label\"]]\n"
   ],
   "id": "b1384ce80fd5b54a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:04:09.615954Z",
     "start_time": "2025-07-09T18:04:09.545573Z"
    }
   },
   "cell_type": "code",
   "source": "from torchview import draw_graph\n",
   "id": "2c3eb8b37513aabc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T19:45:38.112044Z",
     "start_time": "2025-07-09T19:09:24.579602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 25\n",
    "for i, df in enumerate([right_arm_df, left_arm_df, left_leg_df, right_leg_df]):\n",
    "    all_X, all_y = [], []\n",
    "    for subj in df['subject'].unique():\n",
    "        df_sub = df[df['subject'] == subj].reset_index(drop=True)\n",
    "        X_sub, y_sub = create_sequences(df_sub, [c for c in df.columns if c not in ['sbj_id', 'subject', 'label', 'label_code', 'remapped_label']], 'remapped_label', WINDOW_SIZE, STEP_SIZE)\n",
    "        all_X.append(X_sub)\n",
    "        all_y.append(y_sub)\n",
    "    X = np.vstack(all_X)\n",
    "    y = np.hstack(all_y)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(SensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(SensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    d = 3\n",
    "    num_classes = 18\n",
    "    contrastive_loader = DataLoader(ContrastiveDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    contrastive_model = DeepConvLSTM_contrastive(d).to(device)\n",
    "    contrastive_optimizer = torch.optim.Adam(contrastive_model.parameters(), lr=1e-3)\n",
    "    contrastive_criterion = NTXentLoss(device=device, batch_size=batch_size, temperature=0.5)\n",
    "    model_graph = draw_graph(contrastive_model, graph_name=\"ContrastModelTransformer\", input_size=(batch_size, 50, 3), device='cpu', roll=True, save_graph=True)\n",
    "\n",
    "    CONTRASTIVE_EPOCHS = 10\n",
    "    for epoch in range(1, CONTRASTIVE_EPOCHS + 1):\n",
    "        contrastive_model.train()\n",
    "        total_loss = 0\n",
    "        for x1, x2, _ in tqdm(contrastive_loader, desc=f\"Epoch {epoch}\"):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "            contrastive_optimizer.zero_grad()\n",
    "\n",
    "            _, proj1 = contrastive_model(x1)\n",
    "            _, proj2 = contrastive_model(x2)\n",
    "\n",
    "            loss = contrastive_criterion(proj1, proj2)\n",
    "\n",
    "            loss.backward()\n",
    "            contrastive_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x1.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(contrastive_loader.dataset)\n",
    "        print(f\"Epoch {epoch}/{CONTRASTIVE_EPOCHS} - Contrastive Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    for param in contrastive_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    classifier = TransformerClassifier(\n",
    "    input_dim=128,\n",
    "    num_classes=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4) # Transformers often prefer a smaller learning rate\n",
    "    classifier_criterion = nn.CrossEntropyLoss()\n",
    "    model_graph = draw_graph(contrastive_model, graph_name=\"ClassifierModelTransformer\", input_size=(batch_size, 1, 128), device='cpu', roll=True, save_graph=True)\n",
    "\n",
    "    FINETUNE_EPOCHS = 20\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(1, FINETUNE_EPOCHS + 1):\n",
    "        contrastive_model.eval() # Backbone is in eval mode\n",
    "        classifier.train()       # Classifier is in train mode\n",
    "        train_loss = 0\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            classifier_optimizer.zero_grad()\n",
    "\n",
    "            # Get features from the frozen contrastive model\n",
    "            # The permute operation is correctly handled inside its forward pass\n",
    "            with torch.no_grad():\n",
    "                features, _ = contrastive_model(Xb)\n",
    "\n",
    "            # Train the classifier\n",
    "            preds = classifier(features)\n",
    "            loss = classifier_criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            classifier_optimizer.step()\n",
    "            train_loss += loss.item() * Xb.size(0)\n",
    "        train_losses.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "        contrastive_model.eval()\n",
    "        classifier.eval()\n",
    "        val_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                features, _ = contrastive_model(Xb)\n",
    "                preds = classifier(features)\n",
    "                val_loss += classifier_criterion(preds, yb).item() * Xb.size(0)\n",
    "                correct += (preds.argmax(1) == yb).sum().item()\n",
    "        val_losses.append(val_loss / len(val_loader.dataset))\n",
    "        print(f\"Epoch {epoch}/{FINETUNE_EPOCHS} - Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}, Acc: {correct / len(val_loader.dataset):.4f}\")\n",
    "    torch.save(contrastive_model, f\"models/DeepConvContrast/2_{i}_contrast.pt\")\n",
    "    torch.save(classifier, f\"models/DeepConvContrast/2_{i}_classifier.pt\")\n",
    "\n",
    "\n"
   ],
   "id": "b5283effe513944a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (82157, 50, 3), y shape: (82157,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1026/1026 [03:15<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Contrastive Loss: 3.2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1026/1026 [04:00<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Contrastive Loss: 3.1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1026/1026 [03:11<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Contrastive Loss: 3.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1026/1026 [03:34<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Contrastive Loss: 3.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1026/1026 [03:05<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Contrastive Loss: 3.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1026/1026 [03:13<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Contrastive Loss: 3.1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1026/1026 [03:54<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Contrastive Loss: 3.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1026/1026 [03:32<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Contrastive Loss: 3.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1026/1026 [03:49<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Contrastive Loss: 3.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1026/1026 [04:20<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Contrastive Loss: 3.1089\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchgraph see error message",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/torchview.py:261\u001B[39m, in \u001B[36mforward_prop\u001B[39m\u001B[34m(model, x, device, model_graph, mode, **kwargs)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m     _ = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, Mapping):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/recorder_tensor.py:154\u001B[39m, in \u001B[36mmodule_forward_wrapper.<locals>._module_forward_wrapper\u001B[39m\u001B[34m(mod, *args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# TODO: check if output contains RecorderTensor\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# this seems not to be necessary so far\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m out = \u001B[43m_orig_module_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    156\u001B[39m model_graph.context_tracker[\u001B[33m'\u001B[39m\u001B[33mcurrent_depth\u001B[39m\u001B[33m'\u001B[39m] = cur_depth\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mDeepConvLSTM_contrastive.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     21\u001B[39m x = x.permute(\u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m x = \u001B[38;5;28mself\u001B[39m.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m); x = \u001B[38;5;28mself\u001B[39m.pool(x)\n\u001B[32m     23\u001B[39m x = \u001B[38;5;28mself\u001B[39m.relu(\u001B[38;5;28mself\u001B[39m.conv2(x)); x = \u001B[38;5;28mself\u001B[39m.pool(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/recorder_tensor.py:154\u001B[39m, in \u001B[36mmodule_forward_wrapper.<locals>._module_forward_wrapper\u001B[39m\u001B[34m(mod, *args, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# TODO: check if output contains RecorderTensor\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# this seems not to be necessary so far\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m out = \u001B[43m_orig_module_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    156\u001B[39m model_graph.context_tracker[\u001B[33m'\u001B[39m\u001B[33mcurrent_depth\u001B[39m\u001B[33m'\u001B[39m] = cur_depth\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001B[39m, in \u001B[36mConv1d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    374\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001B[39m, in \u001B[36mConv1d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    359\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv1d(\n\u001B[32m    360\u001B[39m         F.pad(\n\u001B[32m    361\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    368\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    369\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m370\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/recorder_tensor.py:250\u001B[39m, in \u001B[36mRecorderTensor.__torch_function__\u001B[39m\u001B[34m(cls, func, types, args, kwargs)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    248\u001B[39m     \u001B[38;5;66;03m# use original torch_function; otherwise,\u001B[39;00m\n\u001B[32m    249\u001B[39m     \u001B[38;5;66;03m# it leads to infinite recursive call of torch_function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     out = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__torch_function__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[38;5;66;03m# if no RecorderTensor is found in input or output\u001B[39;00m\n\u001B[32m    253\u001B[39m \u001B[38;5;66;03m# dont create any node, give the result only\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torch/_tensor.py:1668\u001B[39m, in \u001B[36mTensor.__torch_function__\u001B[39m\u001B[34m(cls, func, types, args, kwargs)\u001B[39m\n\u001B[32m   1667\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _C.DisableTorchFunctionSubclass():\n\u001B[32m-> \u001B[39m\u001B[32m1668\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m get_default_nowrap_functions():\n",
      "\u001B[31mRuntimeError\u001B[39m: Given groups=1, weight of size [64, 3, 5], expected input[64, 128, 1] to have 3 channels, but got 128 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 62\u001B[39m\n\u001B[32m     60\u001B[39m classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=\u001B[32m1e-4\u001B[39m) \u001B[38;5;66;03m# Transformers often prefer a smaller learning rate\u001B[39;00m\n\u001B[32m     61\u001B[39m classifier_criterion = nn.CrossEntropyLoss()\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m model_graph = \u001B[43mdraw_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontrastive_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mClassifierModelTransformer\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroll\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_graph\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m FINETUNE_EPOCHS = \u001B[32m20\u001B[39m\n\u001B[32m     66\u001B[39m train_losses, val_losses = [], []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/torchview.py:225\u001B[39m, in \u001B[36mdraw_graph\u001B[39m\u001B[34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, collect_attributes, **kwargs)\u001B[39m\n\u001B[32m    216\u001B[39m input_recorder_tensor, kwargs_record_tensor, input_nodes = process_input(\n\u001B[32m    217\u001B[39m     input_data, input_size, kwargs, device, dtypes, collect_attributes\n\u001B[32m    218\u001B[39m )\n\u001B[32m    220\u001B[39m model_graph = ComputationGraph(\n\u001B[32m    221\u001B[39m     visual_graph, input_nodes, show_shapes, expand_nested,\n\u001B[32m    222\u001B[39m     hide_inner_tensors, hide_module_functions, roll, depth, collect_attributes\n\u001B[32m    223\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m \u001B[43mforward_prop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_recorder_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs_record_tensor\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    230\u001B[39m model_graph.fill_visual_graph()\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m save_graph:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dokumente/Projekte/hasca-wear/venv/lib/python3.12/site-packages/torchview/torchview.py:269\u001B[39m, in \u001B[36mforward_prop\u001B[39m\u001B[34m(model, x, device, model_graph, mode, **kwargs)\u001B[39m\n\u001B[32m    267\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mUnknown input type\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    268\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    270\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFailed to run torchgraph see error message\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    271\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    273\u001B[39m     model.train(saved_model_mode)\n",
      "\u001B[31mRuntimeError\u001B[39m: Failed to run torchgraph see error message"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a7ac37b3d18d252c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
