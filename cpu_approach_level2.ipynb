{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:23:33.000392Z",
     "start_time": "2025-06-25T15:23:29.300394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, math, random, json, warnings, itertools, time, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "RAW_DIR   = Path('data')\n",
    "TRAIN_DIR = RAW_DIR / 'dataset_by_location'\n",
    "TEST_CSV  = RAW_DIR / 'test.csv'                   # public test\n",
    "LOCATION_IDS = [\"right_arm\", \"left_arm\", \"right_leg\", \"left_leg\"]\n",
    "WORK_DIR = Path('work2')\n",
    "WIN_SIZE  = 50\n",
    "STRIDE    = 25\n",
    "BATCH_SZ  = 256\n",
    "EPOCHS    = 20\n",
    "EMB_DIM   = 64\n",
    "LR        = 1e-3\n",
    "NUM_WORKERS = 4\n",
    "DEVICE    = 'cpu'#cuda' if torch.cuda.is_available() else 'cpu'\n",
    "label_map = {\n",
    "    'null': 0,'jogging': 1,'jogging (rotating arms)': 2,'jogging (skipping)': 3,'jogging (sidesteps)': 4,'jogging (butt-kicks)': 5,\n",
    "    'stretching (triceps)': 6,'stretching (lunging)': 7,'stretching (shoulders)': 8,'stretching (hamstrings)': 9,'stretching (lumbar rotation)': 10,\n",
    "    'push-ups': 11,'push-ups (complex)': 12,'sit-ups': 13,'sit-ups (complex)': 14,'burpees': 15,'lunges': 16,'lunges (complex)': 17,'bench-dips': 18\n",
    "}\n",
    "loc = \"right_arm\""
   ],
   "id": "c0e8c71361793e48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 17:23:30.115234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750865010.130127   75039 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750865010.134542   75039 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750865010.145391   75039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865010.145410   75039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865010.145412   75039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865010.145413   75039 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-25 17:23:30.149372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:23:34.523311Z",
     "start_time": "2025-06-25T15:23:34.505226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score\n",
    "from ast import literal_eval\n",
    "from numpy import nan\n",
    "\n",
    "class WearModelBuilder:\n",
    "    def __init__(self, num_classes=19):\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {\n",
    "            'null': 0, 'jogging': 1, 'jogging (rotating arms)': 2,\n",
    "            'jogging (skipping)': 3, 'jogging (butt-kicks)': 4,\n",
    "            'stretching (triceps)': 5, 'stretching (lunging)': 6,\n",
    "            'stretching (shoulders)': 7, 'stretching (hamstrings)': 8,\n",
    "            'stretching (lumbar rotation)': 9, 'push-ups': 10,\n",
    "            'push-ups (complex)': 11, 'sit-ups': 12,\n",
    "            'sit-ups (complex)': 13, 'burpees': 14,\n",
    "            'lunges': 15, 'lunges (complex)': 16,\n",
    "            'bench-dips': 17\n",
    "        }\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            df = pd.read_csv(f)\n",
    "        df['x_axis'] = df['x_axis'].apply(lambda row: eval(row))\n",
    "        df['y_axis'] = df['y_axis'].apply(lambda row: eval(row))\n",
    "        df['z_axis'] = df['z_axis'].apply(lambda row: eval(row))\n",
    "        df['sbj_id'] = df['sbj_id'].astype(int)\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        return df\n",
    "\n",
    "    def prepare_datasets(self, df, test_size=0.2, random_state=42):\n",
    "        subjects = df['sbj_id'].unique()\n",
    "        train_subjects, test_subjects = train_test_split(\n",
    "            subjects, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        train_df = df[df['sbj_id'].isin(train_subjects)]\n",
    "        test_df = df[df['sbj_id'].isin(test_subjects)]\n",
    "\n",
    "        return train_df, test_df\n",
    "\n",
    "    def build_conv_lstm_model(self, input_shape):\n",
    "        \"\"\"Build a ConvLSTM model based on challenge recommendations\"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Conv block 1\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Conv block 2\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(128, return_sequences=False)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # Output\n",
    "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_model(self, train_df, test_df, epochs=50, batch_size=32):\n",
    "        \"\"\"Train a model on the given data\"\"\"\n",
    "        train_df[\"x_axis\"]=train_df[\"x_axis\"].apply(lambda row: np.array(row))\n",
    "        train_df[\"y_axis\"]=train_df[\"y_axis\"].apply(lambda row: np.array(row))\n",
    "        train_df[\"z_axis\"]=train_df[\"z_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"x_axis\"]=test_df[\"x_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"y_axis\"]=test_df[\"y_axis\"].apply(lambda row: np.array(row))\n",
    "        test_df[\"z_axis\"]=test_df[\"z_axis\"].apply(lambda row: np.array(row))\n",
    "        X_train = train_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "        y_train = train_df['label'].values\n",
    "        X_test = test_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "        y_test = test_df['label'].values\n",
    "        train_array = np.zeros((X_train.shape[0], 3, 50))\n",
    "        for i in range(X_train.shape[0]):\n",
    "            for j in range(X_train.shape[1]):\n",
    "                train_array[i][j]=np.array(X_train[i][j])\n",
    "        test_array = np.zeros((X_test.shape[0], 3, 50))\n",
    "        for i in range(X_test.shape[0]):\n",
    "            for j in range(X_test.shape[1]):\n",
    "                test_array[i][j]=np.array(X_test[i][j])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(np.array([x.flatten() for x in train_array])).reshape(train_array.shape)\n",
    "        X_test = scaler.transform(np.array([x.flatten() for x in test_array])).reshape(test_array.shape)\n",
    "\n",
    "        y_train = to_categorical(y_train, num_classes=self.num_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=self.num_classes)\n",
    "\n",
    "        model = self.build_conv_lstm_model(input_shape=(3, 50))\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_true = test_df['label'].values\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "        return model, history, f1"
   ],
   "id": "98a96c44aabfa38b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T22:49:39.959214Z",
     "start_time": "2025-06-24T22:48:58.416185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wearModelBuilder =  WearModelBuilder()\n",
    "df = wearModelBuilder.load_data(TRAIN_DIR/'left_arm_windows.csv')\n",
    "result = wearModelBuilder.prepare_datasets(df)\n",
    "model1 = wearModelBuilder.build_conv_lstm_model(result[0].shape)"
   ],
   "id": "c6f2d15ef9dbf007",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:48:01.262316Z",
     "start_time": "2025-06-24T21:47:19.980642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wearModelBuilder =  WearModelBuilder()\n",
    "df = wearModelBuilder.load_data(TRAIN_DIR/'right_arm_windows.csv')\n",
    "result = wearModelBuilder.prepare_datasets(df)\n",
    "model1 = wearModelBuilder.build_conv_lstm_model(result[0].shape)\n",
    "model1"
   ],
   "id": "afbe32015e861b5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_7, built=True>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for entry in LOCATION_IDS[1:]:\n",
    "    wear_model_builder = WearModelBuilder()\n",
    "    df = wear_model_builder.load_data(TRAIN_DIR/f\"{entry}_windows.csv\")\n",
    "    result = wearModelBuilder.prepare_datasets(df)\n",
    "    wear_model_builder.build_conv_lstm_model(result[0].shape)\n",
    "    model = wear_model_builder.train_model(result[0], result[1])\n",
    "    model[0].save(WORK_DIR/f\"model-{entry}.h5\")"
   ],
   "id": "72cd8cc6bbaeb4c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3403/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6302 - loss: 1.0264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6302 - loss: 1.0264 - val_accuracy: 0.6472 - val_loss: 1.1035\n",
      "Epoch 26/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6288 - loss: 1.0298 - val_accuracy: 0.6285 - val_loss: 1.1511\n",
      "Epoch 27/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6283 - loss: 1.0250 - val_accuracy: 0.6511 - val_loss: 1.1155\n",
      "Epoch 28/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6368 - loss: 1.0146 - val_accuracy: 0.6549 - val_loss: 1.1058\n",
      "Epoch 29/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6324 - loss: 1.0200 - val_accuracy: 0.6421 - val_loss: 1.1339\n",
      "Epoch 30/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 5ms/step - accuracy: 0.6332 - loss: 1.0143 - val_accuracy: 0.6509 - val_loss: 1.1393\n",
      "Epoch 31/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 7ms/step - accuracy: 0.6359 - loss: 1.0069 - val_accuracy: 0.6431 - val_loss: 1.1434\n",
      "Epoch 32/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6ms/step - accuracy: 0.6351 - loss: 1.0109 - val_accuracy: 0.6459 - val_loss: 1.1272\n",
      "Epoch 33/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 5ms/step - accuracy: 0.6377 - loss: 1.0044 - val_accuracy: 0.6515 - val_loss: 1.1284\n",
      "Epoch 34/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 5ms/step - accuracy: 0.6384 - loss: 1.0029 - val_accuracy: 0.6463 - val_loss: 1.1514\n",
      "Epoch 35/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 5ms/step - accuracy: 0.6380 - loss: 0.9991 - val_accuracy: 0.6506 - val_loss: 1.1253\n",
      "\u001B[1m924/924\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-Score: 0.5403\n",
      "Epoch 1/50\n",
      "\u001B[1m3399/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.4836 - loss: 1.5823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 4ms/step - accuracy: 0.4837 - loss: 1.5817 - val_accuracy: 0.5363 - val_loss: 1.5846\n",
      "Epoch 2/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.5649 - loss: 1.2518 - val_accuracy: 0.5644 - val_loss: 1.6420\n",
      "Epoch 3/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.5851 - loss: 1.1823 - val_accuracy: 0.5545 - val_loss: 1.6064\n",
      "Epoch 4/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 4ms/step - accuracy: 0.5902 - loss: 1.1551 - val_accuracy: 0.5618 - val_loss: 1.6509\n",
      "Epoch 5/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.5993 - loss: 1.1282 - val_accuracy: 0.5810 - val_loss: 1.6265\n",
      "Epoch 6/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 4ms/step - accuracy: 0.6039 - loss: 1.1144 - val_accuracy: 0.5663 - val_loss: 1.7094\n",
      "Epoch 7/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6043 - loss: 1.1064 - val_accuracy: 0.5748 - val_loss: 1.6976\n",
      "Epoch 8/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 5ms/step - accuracy: 0.6102 - loss: 1.0930 - val_accuracy: 0.5611 - val_loss: 1.7430\n",
      "Epoch 9/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6113 - loss: 1.0778 - val_accuracy: 0.5753 - val_loss: 1.7194\n",
      "Epoch 10/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6147 - loss: 1.0716 - val_accuracy: 0.5703 - val_loss: 1.7724\n",
      "Epoch 11/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 4ms/step - accuracy: 0.6152 - loss: 1.0675 - val_accuracy: 0.5725 - val_loss: 1.7250\n",
      "\u001B[1m924/924\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-Score: 0.3804\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = wearModelBuilder.train_model(result[0],result[1])",
   "id": "68f559b27816d329",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6405 - loss: 1.0419 - val_accuracy: 0.6027 - val_loss: 1.2758\n",
      "Epoch 37/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6421 - loss: 1.0335 - val_accuracy: 0.5988 - val_loss: 1.2736\n",
      "Epoch 38/50\n",
      "\u001B[1m3407/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6415 - loss: 1.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6415 - loss: 1.0319 - val_accuracy: 0.6027 - val_loss: 1.2624\n",
      "Epoch 39/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6425 - loss: 1.0348 - val_accuracy: 0.5913 - val_loss: 1.2815\n",
      "Epoch 40/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6463 - loss: 1.0268 - val_accuracy: 0.6003 - val_loss: 1.2682\n",
      "Epoch 41/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6487 - loss: 1.0237 - val_accuracy: 0.5941 - val_loss: 1.2879\n",
      "Epoch 42/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6491 - loss: 1.0208 - val_accuracy: 0.5953 - val_loss: 1.2678\n",
      "Epoch 43/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6458 - loss: 1.0239 - val_accuracy: 0.5928 - val_loss: 1.2925\n",
      "Epoch 44/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6469 - loss: 1.0213 - val_accuracy: 0.6016 - val_loss: 1.2789\n",
      "Epoch 45/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6472 - loss: 1.0200 - val_accuracy: 0.5992 - val_loss: 1.2866\n",
      "Epoch 46/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 4ms/step - accuracy: 0.6498 - loss: 1.0139 - val_accuracy: 0.5975 - val_loss: 1.2792\n",
      "Epoch 47/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 4ms/step - accuracy: 0.6454 - loss: 1.0153 - val_accuracy: 0.5901 - val_loss: 1.3005\n",
      "Epoch 48/50\n",
      "\u001B[1m3409/3409\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 4ms/step - accuracy: 0.6497 - loss: 1.0132 - val_accuracy: 0.5898 - val_loss: 1.2990\n",
      "\u001B[1m924/924\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1ms/step\n",
      "Macro F1-Score: 0.5276\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T22:05:56.934369Z",
     "start_time": "2025-06-24T22:05:55.737408Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "def88cdff7e6dcb7",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T22:11:05.396882Z",
     "start_time": "2025-06-24T22:11:05.389004Z"
    }
   },
   "cell_type": "code",
   "source": "test_data",
   "id": "7c58b5e05aa93215",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          id  sbj_id sensor_location  \\\n",
       "0          0      22       right_arm   \n",
       "1          1      22       right_arm   \n",
       "2          2      23       right_arm   \n",
       "3          3      23       right_arm   \n",
       "4          4      23       right_arm   \n",
       "...      ...     ...             ...   \n",
       "48931  48931      23        left_leg   \n",
       "48932  48932      25        left_leg   \n",
       "48933  48933      24        left_leg   \n",
       "48934  48934      23        left_leg   \n",
       "48935  48935      23        left_leg   \n",
       "\n",
       "                                                  x_axis  \\\n",
       "0      [0.2428425714285714, 0.2134530714285714, 0.174...   \n",
       "1      [0.6752909910531751, 0.4247757439107542, 0.715...   \n",
       "2      [0.04528874289978267, 0.02636863686942854, -0....   \n",
       "3      [-0.4294609406487871, -0.6656510068112931, -0....   \n",
       "4      [2.476312835574577, 1.6733647285880453, 1.7557...   \n",
       "...                                                  ...   \n",
       "48931  [1.0630785714285715, 1.0709205714285714, 0.933...   \n",
       "48932  [0.834704380952381, 0.9560397142857142, 0.7693...   \n",
       "48933  [1.060091142857143, 1.0703054285714286, 1.0746...   \n",
       "48934  [-0.9559132857142858, -0.0778062857142857, -0....   \n",
       "48935  [0.97765, 0.9826858571428572, 0.98731071428571...   \n",
       "\n",
       "                                                  y_axis  \\\n",
       "0      [0.8463437142857143, 0.854221738095238, 0.8428...   \n",
       "1      [-0.9366750995461923, -0.6223881559309377, -0....   \n",
       "2      [0.5794056316368844, 0.5853422923083632, 0.646...   \n",
       "3      [-0.147632980836954, -0.014413570611691117, -0...   \n",
       "4      [-0.5983137218068052, -0.10164738944146791, -0...   \n",
       "...                                                  ...   \n",
       "48931  [0.1038264761904761, 0.1515437142857142, 0.190...   \n",
       "48932  [0.4938431904761905, -0.2392702857142856, -0.9...   \n",
       "48933  [0.0607051428571428, 0.0170856904761904, 0.008...   \n",
       "48934  [-0.5109039047619047, 2.9042674285714285, 0.89...   \n",
       "48935  [-0.185416, -0.1850214761904762, -0.1988123809...   \n",
       "\n",
       "                                                  z_axis  \n",
       "0      [-0.4574545714285714, -0.4578691904761905, -0....  \n",
       "1      [-0.6175092749440796, -0.5824192453686647, -0....  \n",
       "2      [0.5778143538332879, 0.5922718075175648, 0.873...  \n",
       "3      [-1.0071089537700262, -0.8174943548296542, -0....  \n",
       "4      [-2.0541462483911856, -1.6842387016850981, -1....  \n",
       "...                                                  ...  \n",
       "48931  [0.4514348571428571, 0.4639102857142857, 0.422...  \n",
       "48932  [0.0447854047619047, -0.0320387142857142, -0.2...  \n",
       "48933  [-0.1507257142857143, -0.1675727857142857, -0....  \n",
       "48934  [0.1249424761904761, 0.2449837142857142, 0.542...  \n",
       "48935  [0.2495088571428571, 0.2521657142857142, 0.248...  \n",
       "\n",
       "[48936 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sbj_id</th>\n",
       "      <th>sensor_location</th>\n",
       "      <th>x_axis</th>\n",
       "      <th>y_axis</th>\n",
       "      <th>z_axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>right_arm</td>\n",
       "      <td>[0.2428425714285714, 0.2134530714285714, 0.174...</td>\n",
       "      <td>[0.8463437142857143, 0.854221738095238, 0.8428...</td>\n",
       "      <td>[-0.4574545714285714, -0.4578691904761905, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>right_arm</td>\n",
       "      <td>[0.6752909910531751, 0.4247757439107542, 0.715...</td>\n",
       "      <td>[-0.9366750995461923, -0.6223881559309377, -0....</td>\n",
       "      <td>[-0.6175092749440796, -0.5824192453686647, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>right_arm</td>\n",
       "      <td>[0.04528874289978267, 0.02636863686942854, -0....</td>\n",
       "      <td>[0.5794056316368844, 0.5853422923083632, 0.646...</td>\n",
       "      <td>[0.5778143538332879, 0.5922718075175648, 0.873...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>right_arm</td>\n",
       "      <td>[-0.4294609406487871, -0.6656510068112931, -0....</td>\n",
       "      <td>[-0.147632980836954, -0.014413570611691117, -0...</td>\n",
       "      <td>[-1.0071089537700262, -0.8174943548296542, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>right_arm</td>\n",
       "      <td>[2.476312835574577, 1.6733647285880453, 1.7557...</td>\n",
       "      <td>[-0.5983137218068052, -0.10164738944146791, -0...</td>\n",
       "      <td>[-2.0541462483911856, -1.6842387016850981, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48931</th>\n",
       "      <td>48931</td>\n",
       "      <td>23</td>\n",
       "      <td>left_leg</td>\n",
       "      <td>[1.0630785714285715, 1.0709205714285714, 0.933...</td>\n",
       "      <td>[0.1038264761904761, 0.1515437142857142, 0.190...</td>\n",
       "      <td>[0.4514348571428571, 0.4639102857142857, 0.422...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48932</th>\n",
       "      <td>48932</td>\n",
       "      <td>25</td>\n",
       "      <td>left_leg</td>\n",
       "      <td>[0.834704380952381, 0.9560397142857142, 0.7693...</td>\n",
       "      <td>[0.4938431904761905, -0.2392702857142856, -0.9...</td>\n",
       "      <td>[0.0447854047619047, -0.0320387142857142, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48933</th>\n",
       "      <td>48933</td>\n",
       "      <td>24</td>\n",
       "      <td>left_leg</td>\n",
       "      <td>[1.060091142857143, 1.0703054285714286, 1.0746...</td>\n",
       "      <td>[0.0607051428571428, 0.0170856904761904, 0.008...</td>\n",
       "      <td>[-0.1507257142857143, -0.1675727857142857, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48934</th>\n",
       "      <td>48934</td>\n",
       "      <td>23</td>\n",
       "      <td>left_leg</td>\n",
       "      <td>[-0.9559132857142858, -0.0778062857142857, -0....</td>\n",
       "      <td>[-0.5109039047619047, 2.9042674285714285, 0.89...</td>\n",
       "      <td>[0.1249424761904761, 0.2449837142857142, 0.542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48935</th>\n",
       "      <td>48935</td>\n",
       "      <td>23</td>\n",
       "      <td>left_leg</td>\n",
       "      <td>[0.97765, 0.9826858571428572, 0.98731071428571...</td>\n",
       "      <td>[-0.185416, -0.1850214761904762, -0.1988123809...</td>\n",
       "      <td>[0.2495088571428571, 0.2521657142857142, 0.248...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48936 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "model_dict = {}\n",
    "for entry in LOCATION_IDS:\n",
    "    model_dict[entry] = tf.keras.models.load_model(WORK_DIR/f\"model-{entry}.h5\")\n",
    "test_df[\"x_axis\"]=test_df[\"x_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "test_df[\"y_axis\"]=test_df[\"y_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "test_df[\"z_axis\"]=test_df[\"z_axis\"].apply(lambda row: np.array([float(x) for x in eval(row)]))\n",
    "X_test = test_df[['x_axis', 'y_axis', 'z_axis']].values\n",
    "test_array = np.zeros((X_test.shape[0], 3, 50))\n",
    "for i in range(X_test.shape[0]):\n",
    "    for j in range(X_test.shape[1]):\n",
    "        test_array[i][j]=np.array(X_test[i][j])\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(np.array([x.flatten() for x in test_array])).reshape(test_array.shape)\n",
    "results = []\n",
    "for index in range(len(test_df['id'])):\n",
    "    model = model_dict[test_df[\"sensor_location\"][index]]\n",
    "    result=model.predict(X_test[index].reshape((1, 3, 50)))\n",
    "    results.append(np.argmax(result))\n",
    "\n",
    "result_df = pd.DataFrame([list(test_df[\"id\"].values), list(results)]).transpose().rename(columns={0: 'id', 1: 'label'})\n",
    "result_df.to_csv(\"submission.csv\", index=False)"
   ],
   "id": "114e5b8349e6f738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 28ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 27ms/step\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:25:22.417025Z",
     "start_time": "2025-06-25T15:25:01.509552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_models(location_ids, work_dir):\n",
    "    return {\n",
    "        loc: tf.keras.models.load_model(work_dir / f\"model-{loc}.h5\")\n",
    "        for loc in location_ids\n",
    "    }\n",
    "\n",
    "def preprocess_df(df):\n",
    "    for axis in (\"x_axis\", \"y_axis\", \"z_axis\"):\n",
    "        df[axis] = df[axis].apply(lambda s: np.array([float(x) for x in eval(s)]))\n",
    "    X = df[[\"x_axis\", \"y_axis\", \"z_axis\"]].values\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    arr = np.zeros((N, D, 50), dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(D):\n",
    "            arr[i, j] = X[i, j]\n",
    "    return arr\n",
    "\n",
    "def scale_array(arr):\n",
    "    N, D, L = arr.shape\n",
    "    flat = arr.reshape(N, D * L)\n",
    "    scaled = StandardScaler().fit_transform(flat)\n",
    "    return scaled.reshape(N, D, L)\n",
    "\n",
    "def batch_predict(df, X_scaled, models):\n",
    "    results = np.empty(len(df), dtype=int)\n",
    "\n",
    "    for loc, model in models.items():\n",
    "        mask = (df[\"sensor_location\"] == loc).to_numpy()\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        X_loc = X_scaled[mask]               # Shape: (n_samples_loc, 3, 50)\n",
    "        preds = model.predict(X_loc)         # Batch-Predict\n",
    "        labels = np.argmax(preds, axis=1)    # Bestimme Klassen\n",
    "\n",
    "        results[mask] = labels\n",
    "\n",
    "    return results\n",
    "\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "model_dict = load_models(LOCATION_IDS, WORK_DIR)\n",
    "\n",
    "test_arr  = preprocess_df(test_df)\n",
    "X_scaled  = scale_array(test_arr)\n",
    "\n",
    "labels = batch_predict(test_df, X_scaled, model_dict)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    \"id\":    test_df[\"id\"],\n",
    "    \"label\": labels\n",
    "})\n",
    "result_df.to_csv(\"submission.csv\", index=False)  # has 0.281\n"
   ],
   "id": "b83b0174d069b265",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 17:25:02.724123: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m383/383\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "\u001B[1m383/383\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "\u001B[1m383/383\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "\u001B[1m383/383\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T15:16:35.710999Z",
     "start_time": "2025-06-24T15:16:35.700774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, LSTM, Dense, Dropout,\n",
    "    BatchNormalization, Activation, Add,\n",
    "    GlobalAveragePooling1D, concatenate, MaxPooling1D\n",
    ")\n",
    "\n",
    "class HARModels:\n",
    "    @staticmethod\n",
    "    def build_deepconvlstm(input_shape, num_classes):\n",
    "        \"\"\"Simplified DeepConvLSTM model\"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Conv block 1\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # Conv block 2\n",
    "        x = Conv1D(64, 5, activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # LSTM layer\n",
    "        x = LSTM(128, return_sequences=False)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # Output\n",
    "        outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet(input_shape, num_classes):\n",
    "        \"\"\"1D ResNet model\"\"\"\n",
    "        def residual_block(x, filters, kernel_size=3):\n",
    "            # Shortcut\n",
    "            shortcut = x\n",
    "\n",
    "            # Main path\n",
    "            x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "            # Add shortcut if dimensions match\n",
    "            if shortcut.shape[-1] == filters:\n",
    "                x = Add()([x, shortcut])\n",
    "\n",
    "            x = Activation('relu')(x)\n",
    "            return x\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Initial conv\n",
    "        x = Conv1D(64, 7, padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(3)(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        x = residual_block(x, 64)\n",
    "        x = residual_block(x, 64)\n",
    "        x = residual_block(x, 128)\n",
    "        x = residual_block(x, 128)\n",
    "\n",
    "        # Global pooling and output\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_inception_module(input_tensor, filters):\n",
    "        \"\"\"Single Inception module\"\"\"\n",
    "        # Bottleneck\n",
    "        x_bottleneck = Conv1D(filters, 1, padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "        # Branch 1: 1x1 conv\n",
    "        branch1 = Conv1D(filters, 1, padding='same', activation='relu')(x_bottleneck)\n",
    "\n",
    "        # Branch 2: 3x3 conv\n",
    "        branch2 = Conv1D(filters, 3, padding='same', activation='relu')(x_bottleneck)\n",
    "\n",
    "        # Branch 3: 5x5 conv\n",
    "        branch3 = Conv1D(filters, 5, padding='same', activation='relu')(x_bottleneck)\n",
    "\n",
    "        # Branch 4: Max pooling\n",
    "        branch4 = MaxPooling1D(3, strides=1, padding='same')(input_tensor)\n",
    "        branch4 = Conv1D(filters, 1, padding='same', activation='relu')(branch4)\n",
    "\n",
    "        # Concatenate all branches\n",
    "        return concatenate([branch1, branch2, branch3, branch4], axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_inceptiontime(input_shape, num_classes):\n",
    "        \"\"\"InceptionTime model\"\"\"\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Stack of inception modules\n",
    "        x = HARModels.build_inception_module(inputs, 32)\n",
    "        x = HARModels.build_inception_module(x, 32)\n",
    "        x = HARModels.build_inception_module(x, 64)\n",
    "\n",
    "        # Global pooling and output\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model"
   ],
   "id": "616b01cbf8c5f0c2",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "db8779d3",
   "metadata": {},
   "source": [
    "### 1. Generate per‑location CSVs (adds `subject_id`)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
